{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# imports\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision\n",
    "import torch.utils.data as utils\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils\n",
    "import time\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## hyper-parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "random_seed = 42"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and Loading dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "with open('dataset.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df_uwb_data = data['data_CIR']\n",
    "df_uwb = data['data_t']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "## Train & valdiation & Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_uwb_data.values, df_uwb['NLOS'].values, test_size=0.1, random_state=random_seed, stratify=df_uwb['NLOS'].values)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=random_seed, stratify=y_train)\n",
    "\n",
    "print('X_train_shape {}, y_train_shape {} : '.format(X_train.shape, y_train.shape))\n",
    "# print('X_val_shape {}, y_val_shape {} : '.format(X_val.shape, y_val.shape))\n",
    "print('X_test_shape {}, y_test_shape {} : '.format(X_test.shape, y_test.shape))\n",
    "\n",
    "print(\"Train NLOS 0 count :\", len(y_train[y_train==0])) \n",
    "print(\"Train NLOS 1 count :\", len(y_train[y_train==1])) \n",
    "# print(\"Validation NLOS 0 count :\", len(y_val[y_val==0])) \n",
    "# print(\"Validation NLOS 1 count :\", len(y_val[y_val==1])) \n",
    "print(\"Test NLOS 0 count :\", len(y_test[y_test==0])) \n",
    "print(\"Test NLOS 0 count :\", len(y_test[y_test==1]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train_shape (37800, 1016), y_train_shape (37800,) : \n",
      "X_test_shape (4200, 1016), y_test_shape (4200,) : \n",
      "Train NLOS 0 count : 18900\n",
      "Train NLOS 1 count : 18900\n",
      "Test NLOS 0 count : 2100\n",
      "Test NLOS 0 count : 2100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Amplitude to dB Scale"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "## amplitude to dB\n",
    "X_train_dB = librosa.amplitude_to_db(X_train)\n",
    "X_test_dB = librosa.amplitude_to_db(X_test)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X_train[0])\n",
    "# plt.figure()\n",
    "# plt.plot(X_train[1])\n",
    "# plt.figure()\n",
    "# plt.plot(X_train_dB[0])\n",
    "\n",
    "## nomalization\n",
    "# scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "# X_train_SC = scaler.transform(X_train)\n",
    "# X_test_SC = scaler.transform(X_test)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X_train_SC[0])\n",
    "# plt.figure()\n",
    "# plt.plot(X_train_SC[1])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "idx_los = np.where(y_train==0)[0]\n",
    "idx_nlos = np.where(y_train==1)[0]\n",
    "print(idx_los)\n",
    "print(idx_nlos)\n",
    "X_train_vec = X_train.reshape(-1,1,1016)\n",
    "X_test_vec = X_test.reshape(-1,1,1016)\n",
    "# X_test_vec.shape\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[    7     8     9 ... 37797 37798 37799]\n",
      "[    0     1     2 ... 37787 37791 37796]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "class SiameseNetworkDataset():\n",
    "    \n",
    "    def __init__(self,training_data=None,training_dir=None, setSize=None, transform=None):\n",
    "        # used to prepare the labels and images path\n",
    "        self.training_data = training_data\n",
    "        # self.training_df.columns =[\"image1\",\"image2\",\"label\"]\n",
    "        self.training_dir = training_dir    \n",
    "        self.transform = transform\n",
    "        self.setSize = setSize\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    def __getitem__(self,idx):\n",
    "        # get class index\n",
    "        idx_los = np.where(self.training_dir==0)[0]\n",
    "        idx_nlos = np.where(self.training_dir==1)[0]\n",
    "        # select label\n",
    "        if idx % 2 == 0:\n",
    "            category = random.randint(0,1)\n",
    "            if category == 0:\n",
    "                img1_idx = random.choice(idx_los) # LOS\n",
    "                img2_idx = random.choice(idx_los)\n",
    "                \n",
    "            else:\n",
    "                img1_idx = random.choice(idx_nlos) # NLOS\n",
    "                img2_idx = random.choice(idx_nlos)\n",
    "            img1 = self.training_data[img1_idx] \n",
    "            img2 = self.training_data[img2_idx]\n",
    "            label = 1.0\n",
    "\n",
    "        else:\n",
    "            category2 = random.randint(0,1)\n",
    "            if category2 == 0:\n",
    "                img1_idx = random.choice(idx_los)\n",
    "                img2_idx = random.choice(idx_nlos)\n",
    "            else:\n",
    "                img1_idx = random.choice(idx_nlos)\n",
    "                img2_idx = random.choice(idx_los)\n",
    "            img1 = self.training_data[img1_idx] \n",
    "            img2 = self.training_data[img2_idx]\n",
    "            label = 0.0\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.from_numpy(np.array([label], dtype=np.float32))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "\n",
    "# creates n-way one shot learning evaluation\n",
    "class SiameseEvalSet(Dataset):\n",
    "    def __init__(self, test_data, test_dir, setSize, numWay, transform=None):\n",
    "        self.test_data = test_data\n",
    "        # self.test_df.columns =[\"image1\",\"image2\",\"label\"]\n",
    "        self.test_dir = test_dir    \n",
    "        self.transform = transform\n",
    "        self.setSize = setSize\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    def __getitem__(self, idx):\n",
    "        idx_los = np.where(self.test_dir==0)[0]\n",
    "        idx_nlos = np.where(self.test_dir==1)[0]\n",
    "        # find one main image, 0 is LOS , 1 is NLOS\n",
    "        category = random.randint(0,1)\n",
    "        if category == 0: # if selected label is LOS\n",
    "            mainimg_idx = random.choice(idx_los) # LOS\n",
    "        else:\n",
    "            mainimg_idx = random.choice(idx_nlos) \n",
    "\n",
    "        mainimg = test_data[mainimg_idx]\n",
    "        if self.transfrom:\n",
    "            mainImg = self.transfrom(mainimg)\n",
    "\n",
    "        # find n numbers of distinct images, 1 in the same set as the main\n",
    "        testSet = []\n",
    "        label = np.random.randint(self.numWay) # 0 or 1\n",
    "        for i in range(self.numWay):\n",
    "            if i == label:\n",
    "                if category == 0:\n",
    "                    test_idx = random_choice(idx_los)\n",
    "                else:\n",
    "                    test_idx = random.choice(idx_nlos)\n",
    "            else:\n",
    "                if category == 0:\n",
    "                    test_idx = random.choice(idx_nlos)\n",
    "                else:\n",
    "                    test_idx = random.choice(idx_los)\n",
    "\n",
    "            testImg = test_data[test_idx]\n",
    "            if self.transform:\n",
    "                testImg = self.transform(testImg)\n",
    "            testSet.append(testImg)\n",
    "            # plt.imshow()\n",
    "            return mainImg, testSet, torch.from_numpy(np.array([label], dtype = int))\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# choose a training dataset size and further divide it into train and validation set 80:20\n",
    "dataSize = 100000 # self-defined dataset size\n",
    "TRAIN_PCT = 0.8 # percentage of entire dataset for training\n",
    "train_size = int(dataSize * TRAIN_PCT)\n",
    "val_size = dataSize - train_size\n",
    "print(train_size,val_size)\n",
    "transformations = transforms.Compose(\n",
    "    [transforms.ToTensor()]) \n",
    "\n",
    "CIRdataset = SiameseNetworkDataset(training_data = X_train_vec, training_dir=y_train, setSize=dataSize, transform=transformations)\n",
    "train_set, val_set = random_split(CIRdataset, [train_size, val_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, num_workers=2, shuffle=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80000 20000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# Viewing the sample of images and to check whetehr its loading properly\n",
    "dataloader = DataLoader(CIRdataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=1)\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "image1, image2, label = dataiter.next()\n",
    "\n",
    "print(label)\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(image1.squeeze())\n",
    "plt.subplot(212)\n",
    "plt.plot(image2.squeeze())\n",
    "\n",
    "# concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "# imshow(torchvision.utils.make_grid(concatenated))\n",
    "# print(example_batch[2].numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4d1fa8b3d0>]"
      ]
     },
     "metadata": {},
     "execution_count": 82
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"250.10976pt\" version=\"1.1\" viewBox=\"0 0 388.0125 250.10976\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-09-03T17:56:40.471689</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 250.10976 \nL 388.0125 250.10976 \nL 388.0125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.0125 107.627999 \nL 380.8125 107.627999 \nL 380.8125 8.791635 \nL 46.0125 8.791635 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc75fc91ace\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#mc75fc91ace\" y=\"107.627999\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(58.049432 122.226436)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.203812\" xlink:href=\"#mc75fc91ace\" y=\"107.627999\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(111.660062 122.226436)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.176942\" xlink:href=\"#mc75fc91ace\" y=\"107.627999\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(171.633192 122.226436)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.150073\" xlink:href=\"#mc75fc91ace\" y=\"107.627999\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(231.606323 122.226436)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.123203\" xlink:href=\"#mc75fc91ace\" y=\"107.627999\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(291.579453 122.226436)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.096333\" xlink:href=\"#mc75fc91ace\" y=\"107.627999\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <g transform=\"translate(348.371333 122.226436)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"med5f3c1cb3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#med5f3c1cb3\" y=\"103.135437\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(32.65 106.934655)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#med5f3c1cb3\" y=\"57.067328\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5000 -->\n      <g transform=\"translate(13.5625 60.866546)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#med5f3c1cb3\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10000 -->\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path clip-path=\"url(#peb99304ffb)\" d=\"M 61.230682 102.822173 \nL 61.830413 98.897171 \nL 62.130279 98.141654 \nL 62.430144 101.043944 \nL 62.73001 99.182793 \nL 63.029876 100.850458 \nL 63.329741 98.261431 \nL 63.629607 98.427276 \nL 63.929473 98.934025 \nL 64.229338 102.296997 \nL 64.529204 100.666186 \nL 65.128935 100.914954 \nL 65.428801 99.127511 \nL 66.628264 101.965307 \nL 66.928129 100.878099 \nL 67.227995 102.140365 \nL 67.527861 102.011375 \nL 67.827726 101.587548 \nL 68.127592 100.214719 \nL 68.427457 102.407561 \nL 68.727323 101.937666 \nL 69.027189 100.933381 \nL 69.327054 101.532266 \nL 69.926786 102.085084 \nL 70.226651 99.173579 \nL 70.526517 98.23379 \nL 70.826383 100.610904 \nL 71.426114 101.569121 \nL 71.72598 98.897171 \nL 72.025845 102.315424 \nL 72.325711 101.661257 \nL 72.625577 101.366421 \nL 72.925442 100.159437 \nL 73.225308 98.33514 \nL 73.525174 100.463486 \nL 73.825039 101.301926 \nL 74.124905 101.44013 \nL 74.42477 101.191362 \nL 74.724636 97.745468 \nL 75.024502 101.090013 \nL 75.324367 100.094941 \nL 75.624233 101.513839 \nL 75.924099 101.145294 \nL 76.223964 99.044589 \nL 76.52383 98.786607 \nL 77.123561 102.07587 \nL 77.423427 101.00709 \nL 77.723293 101.357208 \nL 78.023158 100.832031 \nL 78.323024 102.490483 \nL 78.62289 99.523697 \nL 79.222621 101.891598 \nL 79.522487 99.330211 \nL 79.822352 98.399635 \nL 80.422084 101.762607 \nL 80.721949 102.066656 \nL 81.021815 98.99852 \nL 81.32168 101.54148 \nL 81.621546 101.053158 \nL 81.921412 101.992948 \nL 82.221277 100.021232 \nL 82.521143 98.934025 \nL 83.120874 101.882384 \nL 83.42074 102.204861 \nL 83.720606 97.754681 \nL 84.020471 100.758322 \nL 84.320337 100.721468 \nL 84.620203 102.287783 \nL 84.920068 101.090013 \nL 85.219934 101.228217 \nL 85.5198 100.260787 \nL 85.819665 101.154508 \nL 86.119531 101.265071 \nL 86.419397 99.30257 \nL 86.719262 101.486198 \nL 87.019128 102.71161 \nL 87.618859 100.629331 \nL 87.918725 102.601047 \nL 88.21859 98.832675 \nL 88.518456 101.329567 \nL 88.818322 101.228217 \nL 89.118187 99.063016 \nL 89.418053 101.301926 \nL 89.717919 100.878099 \nL 90.017784 101.062372 \nL 90.31765 101.043944 \nL 90.617516 99.09987 \nL 90.917381 100.592477 \nL 91.217247 100.822818 \nL 91.517113 100.868886 \nL 91.816978 99.449988 \nL 92.116844 99.30257 \nL 92.41671 101.707325 \nL 92.716575 101.900811 \nL 93.016441 101.799461 \nL 93.316307 100.656972 \nL 93.616172 101.154508 \nL 93.916038 100.4727 \nL 94.215903 100.620118 \nL 94.515769 101.20979 \nL 94.815635 100.610904 \nL 95.1155 99.247288 \nL 95.415366 99.385493 \nL 95.715232 99.975164 \nL 96.015097 99.063016 \nL 96.314963 102.619474 \nL 96.614829 99.090657 \nL 96.914694 101.163722 \nL 97.514426 99.956737 \nL 97.814291 98.178508 \nL 98.114157 101.20979 \nL 98.414023 100.80439 \nL 98.713888 99.809319 \nL 99.013754 98.399635 \nL 99.31362 99.422347 \nL 99.913351 102.370706 \nL 100.213217 100.859672 \nL 100.513082 101.771821 \nL 100.812948 99.689542 \nL 101.112813 99.514483 \nL 101.412679 100.435845 \nL 101.712545 99.984378 \nL 102.01241 101.983734 \nL 102.312276 98.897171 \nL 102.612142 100.426632 \nL 103.211873 102.536551 \nL 103.511739 101.771821 \nL 103.811604 99.50527 \nL 104.411336 102.693183 \nL 104.711201 100.343709 \nL 105.011067 101.283499 \nL 105.310933 100.832031 \nL 105.610798 102.269356 \nL 105.910664 101.10844 \nL 106.21053 98.76818 \nL 106.810261 102.628687 \nL 107.110127 101.900811 \nL 107.709858 99.063016 \nL 108.309589 101.734966 \nL 108.609455 100.914954 \nL 108.90932 102.287783 \nL 109.209186 98.620762 \nL 109.509052 99.459202 \nL 110.408649 100.90574 \nL 110.708514 98.482558 \nL 111.00838 100.564836 \nL 111.308246 101.237431 \nL 111.608111 101.615189 \nL 111.907977 100.242359 \nL 112.207843 102.048229 \nL 112.507708 99.984378 \nL 112.807574 99.265715 \nL 113.10744 100.90574 \nL 113.407305 101.605975 \nL 113.707171 100.012019 \nL 114.007036 101.071585 \nL 114.306902 100.638545 \nL 114.606768 100.500341 \nL 114.906633 98.399635 \nL 115.206499 97.809963 \nL 115.506365 101.375635 \nL 115.80623 102.877455 \nL 116.106096 99.901455 \nL 116.405962 98.832675 \nL 116.705827 100.610904 \nL 117.005693 100.785963 \nL 117.305559 100.14101 \nL 117.605424 102.094297 \nL 117.90529 100.997876 \nL 118.205156 102.730037 \nL 118.505021 98.473344 \nL 118.804887 100.159437 \nL 119.104753 100.131796 \nL 119.404618 101.596762 \nL 119.704484 98.454917 \nL 120.00435 101.799461 \nL 120.304215 99.680328 \nL 120.604081 102.370706 \nL 120.903946 100.988663 \nL 121.203812 101.84553 \nL 121.503678 101.532266 \nL 121.803543 101.698112 \nL 122.103409 100.537195 \nL 122.403275 102.352279 \nL 122.70314 101.559907 \nL 123.003006 100.233146 \nL 123.302872 100.666186 \nL 123.602737 100.767536 \nL 123.902603 101.910025 \nL 124.202469 99.330211 \nL 124.502334 101.200576 \nL 124.8022 100.223932 \nL 125.102066 100.16865 \nL 125.401931 100.749109 \nL 125.701797 98.381208 \nL 126.001663 99.20122 \nL 126.301528 101.882384 \nL 126.601394 99.726397 \nL 126.90126 99.109084 \nL 127.201125 99.477629 \nL 127.800856 101.034731 \nL 128.100722 101.403276 \nL 128.400588 99.717183 \nL 129.000319 101.357208 \nL 129.300185 100.942595 \nL 129.60005 101.64283 \nL 129.899916 100.859672 \nL 130.199782 101.863957 \nL 130.499647 101.523053 \nL 130.799513 99.20122 \nL 131.69911 102.324638 \nL 131.998976 102.204861 \nL 132.298841 102.518124 \nL 132.598707 99.680328 \nL 132.898573 99.606619 \nL 133.198438 100.951808 \nL 133.498304 101.071585 \nL 133.79817 100.832031 \nL 134.098035 102.131152 \nL 134.397901 100.739895 \nL 134.697766 102.168006 \nL 134.997632 101.965307 \nL 135.597363 101.237431 \nL 135.897229 101.64283 \nL 136.197095 100.80439 \nL 136.49696 101.31114 \nL 136.796826 101.071585 \nL 137.096692 101.053158 \nL 137.396557 98.307499 \nL 137.696423 100.150223 \nL 138.296154 100.491127 \nL 138.895886 101.513839 \nL 139.195751 101.615189 \nL 139.495617 101.191362 \nL 139.795483 100.122582 \nL 140.095348 101.891598 \nL 140.395214 99.514483 \nL 140.695079 101.495412 \nL 140.994945 100.233146 \nL 141.294811 100.610904 \nL 141.594676 98.924811 \nL 141.894542 100.306855 \nL 142.194408 100.832031 \nL 142.494273 98.418062 \nL 142.794139 98.445703 \nL 143.094005 98.676044 \nL 143.39387 102.232502 \nL 143.693736 98.740539 \nL 143.993602 100.749109 \nL 144.293467 100.832031 \nL 144.593333 101.191362 \nL 144.893199 99.118298 \nL 145.193064 98.786607 \nL 145.49293 102.545765 \nL 145.792796 101.44013 \nL 146.092661 101.64283 \nL 146.392527 100.380564 \nL 146.692393 100.196291 \nL 147.292124 100.924167 \nL 147.591989 100.297641 \nL 148.191721 102.17722 \nL 148.491586 99.855387 \nL 148.791452 101.679684 \nL 149.391183 98.961666 \nL 149.691049 100.325282 \nL 149.990915 98.491771 \nL 150.29078 98.648403 \nL 150.590646 100.491127 \nL 150.890512 100.822818 \nL 151.190377 100.785963 \nL 151.490243 102.260143 \nL 151.790109 99.63426 \nL 152.089974 98.178508 \nL 152.38984 99.827746 \nL 152.689706 99.661901 \nL 152.989571 101.292712 \nL 153.289437 101.366421 \nL 153.589303 100.131796 \nL 154.189034 99.284143 \nL 154.488899 100.997876 \nL 154.788765 100.537195 \nL 155.388496 102.029802 \nL 155.988228 100.822818 \nL 156.288093 101.191362 \nL 156.587959 98.66683 \nL 156.887825 102.637901 \nL 157.18769 98.473344 \nL 157.487556 101.246644 \nL 157.787422 100.518768 \nL 158.387153 100.730681 \nL 158.687019 99.109084 \nL 158.986884 98.390421 \nL 159.28675 100.334496 \nL 159.586616 101.523053 \nL 159.886481 100.896527 \nL 160.186347 101.84553 \nL 160.486212 99.459202 \nL 160.786078 101.329567 \nL 161.085944 101.615189 \nL 161.385809 100.601691 \nL 161.685675 101.384848 \nL 161.985541 101.265071 \nL 162.285406 100.417418 \nL 162.585272 98.454917 \nL 162.885138 98.528626 \nL 163.185003 101.891598 \nL 163.484869 101.31114 \nL 163.784735 98.307499 \nL 164.0846 97.082087 \nL 164.384466 99.053802 \nL 164.684332 102.932737 \nL 164.984197 100.150223 \nL 165.284063 99.901455 \nL 165.583929 98.629976 \nL 165.883794 100.666186 \nL 166.18366 101.025517 \nL 166.483526 100.583263 \nL 166.783391 102.287783 \nL 167.682988 100.868886 \nL 167.982854 101.854743 \nL 168.282719 101.080799 \nL 168.582585 102.066656 \nL 168.882451 100.021232 \nL 169.182316 100.610904 \nL 169.482182 99.496056 \nL 169.782048 101.550694 \nL 170.081913 101.817889 \nL 170.381779 100.887313 \nL 170.681645 101.753393 \nL 170.98151 101.624403 \nL 171.281376 102.039016 \nL 171.581242 101.265071 \nL 171.881107 102.545765 \nL 172.180973 99.09987 \nL 172.780704 99.790892 \nL 173.08057 101.725752 \nL 173.380436 100.583263 \nL 173.680301 98.639189 \nL 173.980167 102.71161 \nL 174.579898 100.426632 \nL 174.879764 101.698112 \nL 175.479495 98.427276 \nL 175.779361 100.242359 \nL 176.079226 100.537195 \nL 176.379092 100.159437 \nL 176.678958 101.237431 \nL 176.978823 99.763251 \nL 177.278689 100.739895 \nL 177.578555 102.71161 \nL 177.87842 101.74418 \nL 178.178286 100.260787 \nL 178.478152 101.228217 \nL 178.778017 97.542768 \nL 179.377749 100.822818 \nL 179.677614 101.200576 \nL 179.97748 100.546409 \nL 180.577211 98.289072 \nL 180.877077 98.43649 \nL 181.476808 102.61026 \nL 181.776674 100.712254 \nL 182.076539 100.592477 \nL 182.376405 98.473344 \nL 182.676271 101.228217 \nL 182.976136 98.187722 \nL 183.276002 101.54148 \nL 183.575868 102.121938 \nL 184.175599 100.739895 \nL 184.475465 102.37992 \nL 184.77533 98.206149 \nL 185.075196 101.347994 \nL 185.674927 98.731325 \nL 185.974793 101.578335 \nL 186.274659 101.753393 \nL 186.574524 98.906384 \nL 186.87439 101.587548 \nL 187.174255 101.532266 \nL 188.073852 99.256502 \nL 188.373718 101.74418 \nL 188.673584 102.370706 \nL 189.573181 100.058087 \nL 189.873046 101.062372 \nL 190.172912 99.026161 \nL 190.472778 100.279214 \nL 190.772643 100.214719 \nL 191.072509 101.430917 \nL 191.372375 101.679684 \nL 191.67224 101.569121 \nL 191.972106 99.035375 \nL 192.271972 101.117653 \nL 192.571837 100.555623 \nL 192.871703 101.781034 \nL 193.171569 99.422347 \nL 193.471434 100.37135 \nL 193.7713 99.754037 \nL 194.071165 99.726397 \nL 194.371031 100.214719 \nL 194.670897 102.637901 \nL 194.970762 99.763251 \nL 195.270628 98.23379 \nL 195.570494 101.025517 \nL 195.870359 101.84553 \nL 196.170225 100.850458 \nL 196.470091 101.421703 \nL 196.769956 102.37992 \nL 197.069822 100.288427 \nL 197.369688 101.265071 \nL 197.669553 101.762607 \nL 197.969419 98.841889 \nL 198.269285 101.016304 \nL 198.869016 101.347994 \nL 199.168882 99.044589 \nL 199.468747 100.887313 \nL 199.768613 101.154508 \nL 200.068479 101.725752 \nL 200.368344 100.417418 \nL 200.66821 101.00709 \nL 200.968075 99.228861 \nL 201.267941 100.878099 \nL 201.867672 98.795821 \nL 202.167538 98.279858 \nL 202.467404 101.661257 \nL 202.767269 101.44013 \nL 203.067135 100.878099 \nL 203.367001 100.785963 \nL 203.666866 100.408205 \nL 203.966732 98.703685 \nL 204.266598 101.771821 \nL 204.866329 99.367065 \nL 205.46606 99.625047 \nL 205.765926 98.66683 \nL 206.065792 101.274285 \nL 206.365657 102.250929 \nL 206.665523 100.076514 \nL 206.965388 98.860316 \nL 207.56512 102.776105 \nL 207.864985 99.339424 \nL 208.164851 100.555623 \nL 208.464717 101.00709 \nL 208.764582 102.499697 \nL 209.064448 100.822818 \nL 209.364314 101.357208 \nL 209.664179 99.50527 \nL 209.964045 101.062372 \nL 210.263911 98.943239 \nL 210.563776 98.528626 \nL 210.863642 98.537839 \nL 211.163508 100.758322 \nL 211.463373 100.712254 \nL 211.763239 102.702396 \nL 212.063105 100.16865 \nL 212.36297 98.740539 \nL 212.662836 99.247288 \nL 212.962702 101.87317 \nL 213.262567 100.822818 \nL 213.562433 100.398991 \nL 213.862298 100.214719 \nL 214.162164 101.495412 \nL 214.46203 98.887957 \nL 214.761895 101.136081 \nL 215.061761 101.00709 \nL 215.361627 101.486198 \nL 215.661492 98.86953 \nL 215.961358 100.564836 \nL 216.561089 98.648403 \nL 217.160821 102.149579 \nL 217.460686 101.025517 \nL 217.760552 100.813604 \nL 218.060418 100.850458 \nL 218.360283 100.398991 \nL 218.660149 101.403276 \nL 218.960015 98.66683 \nL 219.25988 98.362781 \nL 219.559746 100.749109 \nL 219.859612 101.900811 \nL 220.159477 101.126867 \nL 220.459343 101.219003 \nL 220.759208 100.408205 \nL 221.059074 100.850458 \nL 221.35894 100.03966 \nL 221.658805 98.46413 \nL 221.958671 101.246644 \nL 222.258537 100.730681 \nL 222.558402 101.707325 \nL 222.858268 100.979449 \nL 223.158134 101.513839 \nL 223.457999 100.878099 \nL 223.757865 101.274285 \nL 224.057731 101.87317 \nL 224.357596 100.933381 \nL 224.657462 98.528626 \nL 224.957328 101.919239 \nL 225.557059 99.514483 \nL 225.856925 98.841889 \nL 226.15679 100.408205 \nL 226.456656 100.012019 \nL 227.056387 98.353567 \nL 227.356253 98.086372 \nL 227.656118 101.062372 \nL 227.955984 102.757678 \nL 228.25585 99.145938 \nL 228.555715 100.104155 \nL 228.855581 99.339424 \nL 229.155447 100.878099 \nL 229.455312 98.758966 \nL 229.755178 101.84553 \nL 230.055044 101.532266 \nL 230.354909 100.721468 \nL 230.654775 98.399635 \nL 230.954641 98.961666 \nL 231.254506 101.20979 \nL 231.554372 101.97452 \nL 231.854238 99.523697 \nL 232.154103 100.620118 \nL 232.453969 100.887313 \nL 232.753835 99.975164 \nL 233.0537 101.200576 \nL 233.353566 101.74418 \nL 233.653431 99.265715 \nL 233.953297 102.416774 \nL 234.253163 99.007734 \nL 234.852894 100.592477 \nL 235.15276 100.795177 \nL 235.452625 98.952452 \nL 235.752491 97.994236 \nL 236.052357 100.933381 \nL 236.652088 102.17722 \nL 237.251819 99.007734 \nL 237.551685 101.652044 \nL 237.851551 101.467771 \nL 238.151416 100.933381 \nL 238.451282 101.145294 \nL 238.751148 100.841245 \nL 239.051013 101.532266 \nL 239.350879 102.720824 \nL 239.650745 100.933381 \nL 239.95061 101.615189 \nL 240.250476 101.062372 \nL 240.550341 100.914954 \nL 240.850207 101.043944 \nL 241.150073 101.403276 \nL 241.449938 99.330211 \nL 241.749804 101.320353 \nL 242.04967 100.352923 \nL 242.349535 102.619474 \nL 242.649401 99.93831 \nL 242.949267 98.390421 \nL 243.249132 99.956737 \nL 243.548998 102.186434 \nL 243.848864 101.117653 \nL 244.148729 101.513839 \nL 244.448595 102.941951 \nL 244.748461 99.864601 \nL 245.048326 101.394062 \nL 245.348192 99.63426 \nL 245.648058 101.384848 \nL 245.947923 102.472056 \nL 246.247789 100.094941 \nL 246.547655 101.00709 \nL 246.84752 102.573406 \nL 247.147386 100.795177 \nL 247.447251 101.854743 \nL 247.747117 100.721468 \nL 248.046983 100.362136 \nL 248.346848 101.016304 \nL 248.646714 101.937666 \nL 248.94658 100.850458 \nL 249.246445 101.882384 \nL 249.546311 101.090013 \nL 249.846177 101.937666 \nL 250.146042 101.54148 \nL 250.445908 101.403276 \nL 250.745774 100.306855 \nL 251.645371 100.739895 \nL 252.245102 99.707969 \nL 252.544968 99.984378 \nL 252.844833 101.504626 \nL 253.144699 101.97452 \nL 253.444564 100.279214 \nL 253.74443 100.693827 \nL 254.044296 100.914954 \nL 254.344161 102.647115 \nL 254.644027 101.265071 \nL 254.943893 98.887957 \nL 255.243758 101.20979 \nL 255.543624 102.416774 \nL 255.84349 100.90574 \nL 256.143355 100.527982 \nL 256.443221 102.499697 \nL 256.743087 99.514483 \nL 257.042952 100.739895 \nL 257.342818 98.878743 \nL 257.642684 100.491127 \nL 257.942549 101.191362 \nL 258.242415 101.154508 \nL 258.542281 100.951808 \nL 258.842146 100.16865 \nL 259.142012 99.947523 \nL 259.441878 101.025517 \nL 259.741743 100.583263 \nL 260.341474 100.288427 \nL 260.64134 101.688898 \nL 260.941206 100.454273 \nL 261.241071 101.578335 \nL 261.540937 99.763251 \nL 261.840803 98.841889 \nL 262.140668 99.468415 \nL 262.440534 99.007734 \nL 262.7404 101.090013 \nL 263.040265 100.205505 \nL 263.340131 101.136081 \nL 263.639997 101.347994 \nL 263.939862 98.851102 \nL 264.239728 101.74418 \nL 264.539594 101.200576 \nL 264.839459 101.016304 \nL 265.139325 98.887957 \nL 265.439191 100.933381 \nL 265.739056 100.785963 \nL 266.038922 100.223932 \nL 266.338788 102.462842 \nL 266.938519 99.873815 \nL 267.238384 101.255858 \nL 267.53825 101.863957 \nL 267.838116 99.09987 \nL 268.137981 100.730681 \nL 268.437847 98.657616 \nL 268.737713 100.767536 \nL 269.037578 96.575338 \nL 269.337444 98.445703 \nL 269.63731 101.126867 \nL 269.937175 102.020588 \nL 270.237041 98.095586 \nL 270.836772 102.472056 \nL 271.136638 102.868242 \nL 271.436504 99.947523 \nL 271.736369 99.588192 \nL 272.036235 99.551338 \nL 272.336101 100.187078 \nL 272.635966 101.320353 \nL 272.935832 100.592477 \nL 273.235697 101.863957 \nL 273.535563 101.080799 \nL 273.835429 101.624403 \nL 274.135294 102.868242 \nL 274.43516 99.053802 \nL 274.735026 103.006446 \nL 275.034891 101.587548 \nL 275.334757 102.361492 \nL 275.634623 99.818533 \nL 275.934488 98.805034 \nL 276.234354 100.988663 \nL 276.53422 101.762607 \nL 276.834085 101.578335 \nL 277.133951 102.149579 \nL 277.433817 100.122582 \nL 277.733682 99.827746 \nL 278.033548 101.652044 \nL 278.633279 102.260143 \nL 278.933145 100.251573 \nL 279.233011 99.164366 \nL 279.532876 100.776749 \nL 280.132607 102.112725 \nL 280.432473 101.292712 \nL 280.732339 102.158793 \nL 281.032204 99.016948 \nL 281.33207 100.159437 \nL 281.631936 98.620762 \nL 281.931801 100.887313 \nL 282.231667 101.283499 \nL 282.531533 100.878099 \nL 282.831398 101.237431 \nL 283.131264 100.380564 \nL 283.43113 100.187078 \nL 283.730995 98.851102 \nL 284.030861 101.808675 \nL 284.330727 101.274285 \nL 284.630592 101.430917 \nL 284.930458 94.575982 \nL 285.530189 48.452591 \nL 285.830055 22.507032 \nL 286.129921 20.747231 \nL 286.429786 65.221383 \nL 286.729652 84.588416 \nL 287.029517 68.289519 \nL 287.329383 60.854126 \nL 287.629249 30.430747 \nL 287.929114 13.284197 \nL 288.22898 42.380815 \nL 288.828711 82.48771 \nL 289.128577 36.889496 \nL 289.428443 44.389384 \nL 289.728308 92.595053 \nL 290.028174 74.619277 \nL 290.32804 48.655291 \nL 290.627905 48.203824 \nL 290.927771 83.915822 \nL 291.227637 79.198447 \nL 291.527502 70.076962 \nL 291.827368 76.729197 \nL 292.127234 75.181308 \nL 292.427099 84.348862 \nL 292.726965 88.061951 \nL 293.02683 77.37415 \nL 293.326696 81.326794 \nL 293.626562 81.815116 \nL 293.926427 101.486198 \nL 294.526159 86.919462 \nL 294.826024 85.758546 \nL 295.12589 85.417642 \nL 295.425756 93.332143 \nL 295.725621 96.750397 \nL 296.025487 98.491771 \nL 296.625218 90.531202 \nL 296.925084 93.111016 \nL 297.22495 98.685257 \nL 297.824681 93.857319 \nL 298.124547 94.456205 \nL 298.424412 92.318645 \nL 298.724278 91.102447 \nL 299.024144 91.664477 \nL 299.324009 97.06366 \nL 299.623875 89.480849 \nL 299.92374 86.864181 \nL 300.223606 90.052094 \nL 300.523472 85.417642 \nL 300.823337 86.062595 \nL 301.123203 94.769468 \nL 301.423069 97.39535 \nL 301.722934 98.703685 \nL 302.0228 102.840601 \nL 302.622531 99.726397 \nL 302.922397 98.86953 \nL 303.222263 101.569121 \nL 303.522128 100.076514 \nL 303.821994 99.468415 \nL 304.12186 97.59805 \nL 304.421725 97.045233 \nL 304.721591 99.459202 \nL 305.021457 99.523697 \nL 305.321322 101.44013 \nL 305.621188 97.957381 \nL 305.921054 101.274285 \nL 306.220919 100.90574 \nL 306.520785 102.011375 \nL 306.82065 102.204861 \nL 307.120516 102.048229 \nL 307.420382 98.740539 \nL 308.020113 99.311784 \nL 308.319979 100.841245 \nL 308.91971 102.112725 \nL 309.219576 101.523053 \nL 309.519441 101.817889 \nL 309.819307 100.767536 \nL 310.119173 100.564836 \nL 310.419038 99.901455 \nL 310.718904 98.786607 \nL 311.01877 99.855387 \nL 311.318635 102.757678 \nL 311.618501 98.500985 \nL 311.918367 99.293356 \nL 312.218232 99.698756 \nL 312.518098 99.910669 \nL 313.117829 102.094297 \nL 313.417695 102.121938 \nL 313.71756 98.243003 \nL 314.017426 97.376923 \nL 314.617157 102.416774 \nL 314.917023 100.325282 \nL 315.516754 98.344353 \nL 315.81662 98.23379 \nL 316.116486 98.76818 \nL 316.416351 98.860316 \nL 316.716217 99.256502 \nL 317.016083 101.33878 \nL 317.315948 100.048873 \nL 317.91568 102.416774 \nL 318.515411 100.002805 \nL 318.815277 101.154508 \nL 319.115142 98.510198 \nL 319.415008 101.74418 \nL 319.714873 101.016304 \nL 320.014739 101.182149 \nL 320.314605 100.012019 \nL 320.61447 99.818533 \nL 320.914336 100.454273 \nL 321.214202 100.242359 \nL 321.514067 98.196935 \nL 321.813933 101.357208 \nL 322.113799 102.269356 \nL 322.413664 99.597406 \nL 322.71353 98.556267 \nL 323.313261 101.090013 \nL 323.613127 101.54148 \nL 323.912993 100.223932 \nL 324.212858 100.085728 \nL 324.512724 100.076514 \nL 324.81259 100.463486 \nL 325.112455 99.726397 \nL 325.412321 101.384848 \nL 325.712187 101.578335 \nL 326.012052 102.269356 \nL 326.311918 98.878743 \nL 326.611783 98.519412 \nL 326.911649 99.707969 \nL 327.211515 99.551338 \nL 327.51138 101.255858 \nL 327.811246 100.620118 \nL 328.111112 101.587548 \nL 328.410977 101.292712 \nL 328.710843 98.823462 \nL 329.010709 99.910669 \nL 329.310574 99.477629 \nL 329.61044 101.219003 \nL 329.910306 101.384848 \nL 330.210171 101.329567 \nL 330.510037 98.961666 \nL 330.809903 100.417418 \nL 331.109768 99.763251 \nL 331.409634 100.279214 \nL 331.7095 98.270644 \nL 332.009365 99.689542 \nL 332.309231 100.021232 \nL 332.609097 101.854743 \nL 332.908962 100.850458 \nL 333.208828 101.54148 \nL 333.508693 101.467771 \nL 333.808559 99.247288 \nL 334.108425 100.445059 \nL 334.40829 102.481269 \nL 334.708156 99.072229 \nL 335.008022 101.54148 \nL 335.307887 100.841245 \nL 335.907619 100.749109 \nL 336.207484 99.449988 \nL 336.807216 101.919239 \nL 337.107081 99.809319 \nL 337.706813 101.919239 \nL 338.006678 100.196291 \nL 338.306544 99.588192 \nL 338.60641 100.481914 \nL 338.906275 100.067301 \nL 339.206141 100.445059 \nL 339.506006 101.394062 \nL 340.105738 100.878099 \nL 340.705469 101.090013 \nL 341.005335 101.486198 \nL 341.3052 98.289072 \nL 341.605066 101.84553 \nL 341.904932 101.33878 \nL 342.204797 102.250929 \nL 342.504663 99.145938 \nL 342.804529 99.081443 \nL 343.104394 101.910025 \nL 343.40426 101.652044 \nL 344.003991 100.408205 \nL 344.303857 99.109084 \nL 344.603723 101.54148 \nL 344.903588 101.550694 \nL 345.203454 102.50891 \nL 345.50332 99.901455 \nL 345.803185 101.827102 \nL 346.402916 100.749109 \nL 347.002648 102.306211 \nL 347.302513 101.320353 \nL 347.602379 101.090013 \nL 347.902245 101.532266 \nL 348.20211 99.090657 \nL 348.801842 101.54148 \nL 349.701439 100.537195 \nL 350.001304 102.085084 \nL 350.30117 99.892242 \nL 350.601036 99.256502 \nL 350.900901 99.182793 \nL 351.200767 98.611548 \nL 351.500633 99.40392 \nL 351.800498 101.043944 \nL 352.100364 101.136081 \nL 352.40023 100.656972 \nL 352.700095 100.408205 \nL 353.299826 98.547053 \nL 353.599692 101.569121 \nL 353.899558 100.592477 \nL 354.199423 102.057443 \nL 354.499289 100.647759 \nL 354.799155 100.14101 \nL 355.09902 101.781034 \nL 355.398886 101.099226 \nL 355.698752 102.048229 \nL 355.998617 100.251573 \nL 356.298483 100.979449 \nL 356.898214 101.237431 \nL 357.497946 100.067301 \nL 358.097677 97.109728 \nL 358.397543 100.638545 \nL 358.697408 101.44013 \nL 358.997274 101.062372 \nL 359.297139 99.20122 \nL 359.597005 100.776749 \nL 359.896871 100.970236 \nL 360.196736 102.011375 \nL 360.496602 101.97452 \nL 360.796468 102.739251 \nL 361.096333 99.689542 \nL 361.396199 100.14101 \nL 361.696065 100.859672 \nL 361.99593 100.758322 \nL 362.295796 100.887313 \nL 362.595662 100.030446 \nL 362.895527 98.574694 \nL 363.195393 102.287783 \nL 363.795124 100.592477 \nL 364.09499 101.33878 \nL 364.394856 101.605975 \nL 364.994587 98.795821 \nL 365.294453 100.076514 \nL 365.594318 103.135437 \nL 365.594318 103.135437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 46.0125 107.627999 \nL 46.0125 8.791635 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.8125 107.627999 \nL 380.8125 8.791635 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 46.0125 107.627999 \nL 380.8125 107.627999 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 46.0125 8.791635 \nL 380.8125 8.791635 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 46.0125 226.231635 \nL 380.8125 226.231635 \nL 380.8125 127.395271 \nL 46.0125 127.395271 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#mc75fc91ace\" y=\"226.231635\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(58.049432 240.830072)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.203812\" xlink:href=\"#mc75fc91ace\" y=\"226.231635\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 200 -->\n      <g transform=\"translate(111.660062 240.830072)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.176942\" xlink:href=\"#mc75fc91ace\" y=\"226.231635\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 400 -->\n      <g transform=\"translate(171.633192 240.830072)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.150073\" xlink:href=\"#mc75fc91ace\" y=\"226.231635\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 600 -->\n      <g transform=\"translate(231.606323 240.830072)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.123203\" xlink:href=\"#mc75fc91ace\" y=\"226.231635\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 800 -->\n      <g transform=\"translate(291.579453 240.830072)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.096333\" xlink:href=\"#mc75fc91ace\" y=\"226.231635\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1000 -->\n      <g transform=\"translate(348.371333 240.830072)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#med5f3c1cb3\" y=\"221.739073\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(32.65 225.538292)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#med5f3c1cb3\" y=\"196.831594\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 5000 -->\n      <g transform=\"translate(13.5625 200.630813)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#med5f3c1cb3\" y=\"171.924115\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 10000 -->\n      <g transform=\"translate(7.2 175.723334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#med5f3c1cb3\" y=\"147.016636\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 15000 -->\n      <g transform=\"translate(7.2 150.815855)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#pe57e79cc53)\" d=\"M 61.230682 220.473773 \nL 61.530547 220.772663 \nL 61.830413 221.509924 \nL 62.130279 220.488718 \nL 62.430144 220.802552 \nL 62.73001 220.389088 \nL 63.029876 220.478755 \nL 63.329741 221.081516 \nL 63.629607 220.36418 \nL 63.929473 220.259569 \nL 64.229338 221.445165 \nL 64.529204 220.558458 \nL 64.82907 220.912145 \nL 65.128935 220.473773 \nL 65.428801 221.206053 \nL 65.728667 220.747755 \nL 66.028532 221.046645 \nL 66.328398 220.937052 \nL 66.628264 221.156238 \nL 66.928129 221.589628 \nL 67.227995 220.628199 \nL 67.527861 220.100161 \nL 67.827726 219.068991 \nL 68.127592 220.59831 \nL 68.727323 220.991849 \nL 69.027189 220.488718 \nL 69.327054 220.747755 \nL 69.926786 218.725268 \nL 70.226651 220.239643 \nL 70.526517 220.518607 \nL 70.826383 221.211034 \nL 71.126248 220.747755 \nL 71.426114 221.13133 \nL 71.72598 220.568421 \nL 72.025845 221.06159 \nL 72.325711 220.976904 \nL 72.625577 220.478755 \nL 72.925442 221.126349 \nL 73.225308 221.380405 \nL 73.525174 220.404032 \nL 74.124905 220.473773 \nL 74.42477 221.305683 \nL 74.724636 221.554758 \nL 75.624233 220.399051 \nL 75.924099 220.59831 \nL 76.223964 220.493699 \nL 76.52383 219.821197 \nL 76.823696 220.239643 \nL 77.123561 221.519887 \nL 77.423427 219.387807 \nL 77.723293 220.354217 \nL 78.023158 219.367881 \nL 78.323024 221.380405 \nL 78.62289 221.156238 \nL 78.922755 220.508644 \nL 79.222621 220.86233 \nL 79.522487 218.914565 \nL 79.822352 221.390368 \nL 80.122218 219.826179 \nL 80.422084 220.623218 \nL 80.721949 220.7627 \nL 81.021815 221.59461 \nL 81.32168 219.073973 \nL 81.621546 219.283196 \nL 81.921412 220.892219 \nL 82.221277 219.512344 \nL 82.521143 221.410294 \nL 82.821009 221.29572 \nL 83.120874 220.483736 \nL 83.42074 221.19609 \nL 83.720606 221.23096 \nL 84.020471 220.892219 \nL 84.320337 219.547215 \nL 84.620203 220.254587 \nL 84.920068 220.52857 \nL 85.219934 220.219717 \nL 85.5198 220.782626 \nL 86.119531 221.460109 \nL 86.419397 220.518607 \nL 87.019128 221.49498 \nL 87.318994 220.274513 \nL 87.618859 221.156238 \nL 87.918725 221.425239 \nL 88.21859 221.19609 \nL 88.518456 221.475054 \nL 88.818322 220.473773 \nL 89.418053 221.186127 \nL 89.717919 219.871012 \nL 90.017784 220.59831 \nL 90.617516 220.777644 \nL 90.917381 221.235942 \nL 91.217247 220.932071 \nL 91.517113 220.294439 \nL 91.816978 220.772663 \nL 92.116844 220.742774 \nL 92.41671 220.56344 \nL 92.716575 221.280775 \nL 93.316307 221.216016 \nL 93.616172 221.036682 \nL 94.215903 221.589628 \nL 94.815635 220.513625 \nL 95.1155 220.937052 \nL 95.415366 220.947015 \nL 96.015097 219.871012 \nL 96.314963 220.991849 \nL 96.614829 220.409014 \nL 96.914694 220.79757 \nL 97.21456 220.583366 \nL 97.514426 220.737792 \nL 97.814291 221.026719 \nL 98.114157 220.409014 \nL 98.414023 220.787607 \nL 98.713888 221.380405 \nL 99.613485 221.375424 \nL 99.913351 221.136312 \nL 100.513082 221.06159 \nL 100.812948 221.360479 \nL 101.112813 220.194809 \nL 101.412679 220.947015 \nL 101.712545 221.380405 \nL 102.01241 220.379125 \nL 102.612142 220.951997 \nL 102.912007 220.69794 \nL 103.211873 220.817496 \nL 103.811604 220.418977 \nL 104.11147 220.782626 \nL 104.411336 220.887237 \nL 104.711201 221.420257 \nL 105.011067 220.732811 \nL 105.310933 220.692959 \nL 105.910664 220.239643 \nL 106.21053 220.986867 \nL 106.810261 220.702922 \nL 107.409992 221.216016 \nL 107.709858 220.493699 \nL 108.009723 220.458829 \nL 108.309589 220.732811 \nL 108.90932 220.56344 \nL 109.209186 220.443884 \nL 109.509052 220.46381 \nL 109.808917 220.827459 \nL 110.108783 219.258288 \nL 110.408649 221.39535 \nL 110.708514 220.254587 \nL 111.00838 221.016756 \nL 111.308246 221.171182 \nL 111.608111 220.354217 \nL 111.907977 221.62948 \nL 112.207843 220.707903 \nL 112.507708 220.937052 \nL 112.807574 220.817496 \nL 113.10744 221.161219 \nL 113.407305 220.971923 \nL 113.707171 221.156238 \nL 114.007036 220.508644 \nL 114.306902 220.56344 \nL 114.606768 221.305683 \nL 114.906633 220.219717 \nL 115.206499 220.533551 \nL 115.506365 220.658088 \nL 115.80623 220.478755 \nL 116.106096 220.782626 \nL 116.405962 220.140013 \nL 117.305559 221.285757 \nL 117.605424 220.568421 \nL 117.90529 220.194809 \nL 118.205156 220.882256 \nL 118.505021 220.633181 \nL 118.804887 220.917126 \nL 119.104753 220.483736 \nL 119.404618 220.673033 \nL 119.704484 220.244624 \nL 120.00435 220.543514 \nL 120.304215 220.618236 \nL 120.604081 220.294439 \nL 120.903946 219.088917 \nL 121.203812 220.045364 \nL 121.503678 221.549776 \nL 121.803543 221.29572 \nL 122.103409 220.687977 \nL 122.403275 221.554758 \nL 122.70314 220.005512 \nL 123.003006 219.721567 \nL 123.302872 220.79757 \nL 123.602737 220.648125 \nL 123.902603 220.189828 \nL 124.502334 220.608273 \nL 125.102066 221.639443 \nL 125.401931 220.56344 \nL 125.701797 220.433921 \nL 126.001663 221.400331 \nL 126.301528 221.206053 \nL 126.601394 220.184846 \nL 126.90126 220.483736 \nL 127.201125 220.314365 \nL 127.500991 219.452566 \nL 127.800856 219.268251 \nL 128.100722 220.877274 \nL 128.700453 220.588347 \nL 129.000319 220.932071 \nL 129.300185 221.514906 \nL 129.899916 219.856068 \nL 130.199782 219.477474 \nL 130.499647 221.320627 \nL 130.799513 219.821197 \nL 131.099379 220.468792 \nL 131.399244 220.369162 \nL 131.69911 221.166201 \nL 131.998976 221.151256 \nL 132.298841 220.99683 \nL 132.598707 220.413995 \nL 132.898573 220.772663 \nL 133.79817 219.995549 \nL 134.397901 220.712885 \nL 134.697766 220.493699 \nL 135.297498 220.56344 \nL 135.597363 221.191108 \nL 135.897229 221.011775 \nL 136.197095 219.387807 \nL 136.49696 220.673033 \nL 136.796826 220.070272 \nL 137.096692 218.839842 \nL 137.396557 220.080235 \nL 137.696423 219.960679 \nL 137.996289 220.209754 \nL 138.296154 221.121368 \nL 138.59602 220.379125 \nL 138.895886 220.523588 \nL 139.195751 220.927089 \nL 139.495617 220.294439 \nL 139.795483 220.782626 \nL 140.095348 220.942034 \nL 140.395214 219.871012 \nL 140.695079 220.99683 \nL 140.994945 221.046645 \nL 141.294811 221.49498 \nL 141.594676 221.440183 \nL 142.194408 219.537252 \nL 142.494273 221.216016 \nL 142.794139 220.433921 \nL 143.094005 220.986867 \nL 143.39387 220.448866 \nL 143.693736 220.314365 \nL 143.993602 221.275794 \nL 144.293467 221.440183 \nL 144.893199 220.628199 \nL 145.49293 221.081516 \nL 146.392527 219.93079 \nL 146.692393 219.522307 \nL 146.992258 220.658088 \nL 147.891855 220.573403 \nL 148.191721 221.290738 \nL 148.491586 220.404032 \nL 148.791452 220.438903 \nL 149.091318 220.623218 \nL 149.391183 220.518607 \nL 149.691049 220.59831 \nL 149.990915 221.290738 \nL 150.29078 220.692959 \nL 150.590646 220.917126 \nL 150.890512 220.438903 \nL 151.190377 221.151256 \nL 151.490243 221.091479 \nL 151.790109 219.69666 \nL 152.089974 220.06529 \nL 152.689706 221.609554 \nL 152.989571 220.269532 \nL 153.289437 220.812515 \nL 153.589303 220.687977 \nL 153.889168 220.314365 \nL 154.189034 219.681715 \nL 154.488899 219.861049 \nL 154.788765 221.181145 \nL 155.088631 220.638162 \nL 155.388496 221.056608 \nL 155.988228 219.53227 \nL 156.288093 221.101442 \nL 156.587959 218.979324 \nL 156.887825 221.435202 \nL 157.18769 220.892219 \nL 157.487556 220.887237 \nL 158.087287 220.159939 \nL 158.387153 221.310664 \nL 158.687019 220.877274 \nL 158.986884 221.041664 \nL 159.28675 219.841123 \nL 159.586616 220.722848 \nL 159.886481 220.573403 \nL 160.186347 221.509924 \nL 160.486212 220.538532 \nL 160.786078 220.508644 \nL 161.085944 220.942034 \nL 161.985541 221.171182 \nL 162.585272 220.722848 \nL 162.885138 220.852367 \nL 163.185003 220.707903 \nL 163.484869 221.121368 \nL 163.784735 220.13005 \nL 164.0846 220.468792 \nL 164.384466 219.910864 \nL 164.684332 220.69794 \nL 164.984197 220.473773 \nL 165.284063 220.638162 \nL 165.583929 221.539813 \nL 165.883794 220.578384 \nL 166.18366 220.508644 \nL 166.483526 219.866031 \nL 167.083257 221.325609 \nL 167.383122 220.518607 \nL 167.682988 220.827459 \nL 167.982854 219.93079 \nL 168.282719 220.314365 \nL 168.582585 219.437622 \nL 168.882451 220.169902 \nL 169.182316 219.29814 \nL 169.482182 219.781345 \nL 169.782048 220.807533 \nL 170.081913 220.538532 \nL 170.381779 220.69794 \nL 170.681645 220.643144 \nL 170.98151 219.43264 \nL 171.281376 220.737792 \nL 171.581242 221.509924 \nL 171.881107 220.453847 \nL 172.180973 221.111405 \nL 172.480839 220.279495 \nL 172.780704 221.091479 \nL 173.680301 220.284476 \nL 173.980167 221.181145 \nL 174.280032 220.792589 \nL 174.579898 221.171182 \nL 174.879764 220.727829 \nL 175.179629 220.712885 \nL 175.479495 220.289458 \nL 175.779361 219.228399 \nL 176.079226 220.498681 \nL 176.379092 221.06159 \nL 176.678958 220.503662 \nL 176.978823 221.390368 \nL 177.278689 220.379125 \nL 177.578555 221.240923 \nL 177.87842 221.156238 \nL 178.178286 220.558458 \nL 178.478152 218.969361 \nL 178.778017 220.583366 \nL 179.077883 220.66307 \nL 179.377749 220.917126 \nL 179.677614 220.309384 \nL 179.97748 220.384106 \nL 180.277345 219.273233 \nL 180.877077 220.942034 \nL 181.176942 219.347955 \nL 181.476808 220.827459 \nL 181.776674 220.971923 \nL 182.076539 220.69794 \nL 182.376405 221.559739 \nL 182.676271 219.602011 \nL 182.976136 219.606993 \nL 183.276002 221.106423 \nL 183.575868 220.653107 \nL 184.175599 220.8972 \nL 184.475465 221.265831 \nL 185.075196 219.910864 \nL 185.375062 219.846105 \nL 185.674927 221.370442 \nL 185.974793 220.503662 \nL 186.274659 220.583366 \nL 186.574524 220.493699 \nL 186.87439 220.812515 \nL 187.174255 220.96196 \nL 187.474121 220.389088 \nL 187.773987 220.478755 \nL 188.073852 220.448866 \nL 188.373718 220.169902 \nL 188.673584 221.569702 \nL 188.973449 220.638162 \nL 189.273315 221.156238 \nL 189.573181 221.23096 \nL 189.873046 221.151256 \nL 190.172912 220.653107 \nL 191.072509 220.922108 \nL 191.372375 221.186127 \nL 191.67224 220.423958 \nL 191.972106 221.046645 \nL 192.271972 221.091479 \nL 192.571837 220.568421 \nL 192.871703 220.409014 \nL 193.171569 221.385387 \nL 193.471434 221.240923 \nL 193.7713 220.692959 \nL 194.071165 221.191108 \nL 194.371031 221.445165 \nL 194.970762 219.452566 \nL 195.270628 220.86233 \nL 195.870359 220.314365 \nL 196.170225 220.304402 \nL 196.470091 221.425239 \nL 196.769956 220.548495 \nL 197.069822 220.86233 \nL 197.369688 221.519887 \nL 197.969419 220.722848 \nL 198.269285 221.460109 \nL 199.168882 220.144994 \nL 199.468747 221.345535 \nL 199.768613 220.842404 \nL 200.068479 221.09646 \nL 200.368344 220.244624 \nL 200.66821 221.360479 \nL 200.968075 221.370442 \nL 201.867672 220.174883 \nL 202.167538 220.857348 \nL 202.467404 220.583366 \nL 202.767269 220.79757 \nL 203.067135 221.206053 \nL 203.367001 220.593329 \nL 203.666866 221.310664 \nL 203.966732 220.304402 \nL 204.266598 220.752737 \nL 204.566463 219.945734 \nL 204.866329 220.782626 \nL 205.166195 219.16364 \nL 205.46606 221.315646 \nL 205.765926 221.654388 \nL 206.065792 220.339273 \nL 206.665523 220.673033 \nL 206.965388 221.186127 \nL 207.265254 220.448866 \nL 207.56512 221.151256 \nL 208.164851 221.300701 \nL 208.464717 220.543514 \nL 208.764582 220.887237 \nL 209.064448 221.654388 \nL 209.664179 220.488718 \nL 209.964045 220.872293 \nL 210.263911 220.712885 \nL 210.563776 221.549776 \nL 210.863642 220.433921 \nL 211.163508 220.623218 \nL 211.463373 220.354217 \nL 211.763239 219.437622 \nL 212.063105 220.558458 \nL 212.36297 219.4974 \nL 212.662836 219.636882 \nL 212.962702 220.867311 \nL 213.262567 220.100161 \nL 213.562433 220.234661 \nL 213.862298 220.583366 \nL 214.162164 220.503662 \nL 214.46203 220.822478 \nL 214.761895 220.374143 \nL 215.061761 220.822478 \nL 215.661492 221.290738 \nL 215.961358 220.732811 \nL 216.261224 219.203492 \nL 216.561089 219.661789 \nL 216.860955 219.09888 \nL 217.160821 220.902182 \nL 217.460686 220.623218 \nL 217.760552 220.085216 \nL 218.060418 220.553477 \nL 218.360283 221.460109 \nL 218.660149 220.518607 \nL 218.960015 220.966941 \nL 219.25988 220.473773 \nL 219.559746 221.006793 \nL 219.859612 220.289458 \nL 220.459343 220.976904 \nL 220.759208 220.433921 \nL 221.35894 219.900901 \nL 221.658805 221.136312 \nL 221.958671 220.369162 \nL 222.258537 220.149976 \nL 222.858268 221.201071 \nL 223.457999 221.006793 \nL 223.757865 220.199791 \nL 224.057731 220.752737 \nL 224.357596 220.354217 \nL 224.957328 220.807533 \nL 225.257193 220.638162 \nL 225.557059 219.915846 \nL 225.856925 221.011775 \nL 226.15679 219.083936 \nL 226.456656 220.448866 \nL 226.756521 219.970642 \nL 227.356253 220.991849 \nL 227.656118 220.274513 \nL 227.955984 221.166201 \nL 228.25585 220.349236 \nL 228.555715 220.125068 \nL 228.855581 221.470072 \nL 229.155447 220.453847 \nL 229.455312 221.211034 \nL 229.755178 221.425239 \nL 230.055044 221.345535 \nL 230.354909 220.324328 \nL 230.654775 220.742774 \nL 230.954641 219.741493 \nL 231.254506 221.106423 \nL 231.554372 220.956978 \nL 231.854238 220.384106 \nL 232.154103 220.922108 \nL 232.453969 220.095179 \nL 232.753835 220.937052 \nL 233.0537 221.385387 \nL 233.353566 220.588347 \nL 233.653431 220.538532 \nL 233.953297 220.976904 \nL 234.253163 220.772663 \nL 234.553028 220.922108 \nL 234.852894 221.285757 \nL 235.452625 219.582085 \nL 235.752491 220.682996 \nL 236.052357 220.593329 \nL 236.352222 220.274513 \nL 236.652088 220.707903 \nL 236.951954 221.340553 \nL 237.251819 220.284476 \nL 237.551685 220.643144 \nL 237.851551 220.668051 \nL 238.151416 221.225979 \nL 238.451282 221.310664 \nL 239.051013 220.354217 \nL 239.350879 221.136312 \nL 239.650745 221.43022 \nL 240.250476 220.309384 \nL 240.550341 220.558458 \nL 240.850207 220.508644 \nL 241.150073 220.603292 \nL 241.449938 221.171182 \nL 241.749804 221.006793 \nL 242.04967 220.448866 \nL 242.349535 220.912145 \nL 242.649401 220.767681 \nL 242.949267 221.335572 \nL 243.249132 220.32931 \nL 244.448595 221.435202 \nL 244.748461 220.687977 \nL 245.048326 220.613255 \nL 245.348192 221.011775 \nL 245.648058 220.608273 \nL 245.947923 220.478755 \nL 246.247789 220.633181 \nL 246.547655 221.370442 \nL 246.84752 220.648125 \nL 247.147386 221.270812 \nL 247.447251 220.927089 \nL 247.747117 220.314365 \nL 248.046983 220.737792 \nL 248.346848 219.73153 \nL 248.646714 220.498681 \nL 248.94658 221.719147 \nL 249.246445 221.320627 \nL 249.546311 220.409014 \nL 249.846177 220.56344 \nL 250.146042 220.344254 \nL 250.445908 220.922108 \nL 250.745774 220.99683 \nL 251.345505 221.559739 \nL 251.645371 220.66307 \nL 251.945236 220.623218 \nL 252.544968 221.325609 \nL 253.144699 220.877274 \nL 253.444564 221.09646 \nL 253.74443 220.005512 \nL 254.044296 220.483736 \nL 254.344161 220.623218 \nL 254.644027 220.971923 \nL 254.943893 221.011775 \nL 255.243758 221.475054 \nL 255.543624 220.52857 \nL 255.84349 220.56344 \nL 256.143355 221.390368 \nL 256.443221 221.240923 \nL 256.743087 221.485017 \nL 257.042952 220.438903 \nL 257.342818 220.648125 \nL 257.642684 221.141293 \nL 257.942549 220.100161 \nL 258.242415 220.498681 \nL 258.542281 221.380405 \nL 258.842146 220.060309 \nL 259.142012 220.613255 \nL 259.441878 220.86233 \nL 260.041609 220.932071 \nL 260.341474 219.313085 \nL 260.64134 221.509924 \nL 260.941206 221.19609 \nL 261.241071 221.415276 \nL 261.540937 221.051627 \nL 261.840803 221.106423 \nL 262.140668 220.110124 \nL 262.440534 220.324328 \nL 263.040265 221.544795 \nL 263.340131 219.741493 \nL 263.639997 220.543514 \nL 263.939862 220.932071 \nL 264.539594 220.42894 \nL 264.839459 221.400331 \nL 265.139325 220.52857 \nL 265.439191 220.56344 \nL 265.739056 220.7627 \nL 266.038922 221.320627 \nL 266.338788 220.69794 \nL 266.938519 220.389088 \nL 267.53825 220.59831 \nL 267.838116 220.922108 \nL 268.137981 220.458829 \nL 268.437847 220.86233 \nL 268.737713 220.100161 \nL 269.037578 219.821197 \nL 269.337444 219.337992 \nL 269.63731 220.56344 \nL 269.937175 220.742774 \nL 270.237041 220.394069 \nL 270.536907 220.254587 \nL 270.836772 220.792589 \nL 271.136638 220.214735 \nL 271.436504 220.32931 \nL 271.736369 220.673033 \nL 272.036235 220.468792 \nL 272.336101 221.081516 \nL 272.635966 220.169902 \nL 272.935832 220.932071 \nL 273.235697 219.178584 \nL 273.535563 219.935772 \nL 273.835429 220.26455 \nL 274.135294 220.981886 \nL 274.735026 220.503662 \nL 275.034891 220.458829 \nL 275.334757 220.16492 \nL 275.634623 221.62948 \nL 275.934488 220.548495 \nL 276.234354 220.100161 \nL 276.53422 220.26455 \nL 276.834085 220.648125 \nL 277.133951 220.334291 \nL 277.433817 220.847385 \nL 277.733682 220.857348 \nL 278.033548 220.060309 \nL 278.333414 220.747755 \nL 278.633279 219.427659 \nL 278.933145 219.223418 \nL 279.233011 221.33059 \nL 279.832742 220.658088 \nL 280.132607 220.448866 \nL 280.432473 219.103862 \nL 280.732339 219.83116 \nL 281.032204 219.293159 \nL 281.631936 220.951997 \nL 281.931801 221.216016 \nL 282.531533 221.265831 \nL 282.831398 220.344254 \nL 283.131264 220.548495 \nL 283.43113 221.011775 \nL 283.730995 218.625638 \nL 284.030861 198.091912 \nL 284.330727 148.182306 \nL 284.630592 131.887833 \nL 284.930458 142.518345 \nL 285.530189 135.902919 \nL 285.830055 161.632345 \nL 286.429786 188.198662 \nL 286.729652 192.642156 \nL 287.029517 209.977761 \nL 287.629249 212.383824 \nL 287.929114 211.018894 \nL 288.528846 198.405747 \nL 288.828711 210.296577 \nL 289.128577 214.207051 \nL 289.728308 218.790027 \nL 290.028174 211.307821 \nL 290.32804 208.289034 \nL 290.627905 214.954276 \nL 290.927771 218.780064 \nL 291.227637 217.405172 \nL 291.527502 216.797429 \nL 291.827368 218.690398 \nL 292.127234 218.944454 \nL 292.427099 217.021596 \nL 292.726965 213.843402 \nL 293.02683 214.276792 \nL 293.326696 217.783765 \nL 293.626562 216.413854 \nL 293.926427 215.796149 \nL 294.226293 218.157378 \nL 294.526159 219.537252 \nL 294.826024 219.995549 \nL 295.12589 218.102581 \nL 295.425756 219.044084 \nL 295.725621 219.482455 \nL 296.025487 219.323048 \nL 296.325353 217.743913 \nL 296.625218 219.467511 \nL 296.925084 219.960679 \nL 297.22495 219.935772 \nL 297.524815 218.2321 \nL 297.824681 218.252026 \nL 298.424412 216.792448 \nL 298.724278 216.777503 \nL 299.324009 220.947015 \nL 299.623875 219.826179 \nL 299.92374 220.105142 \nL 300.223606 220.917126 \nL 300.523472 219.606993 \nL 300.823337 216.56828 \nL 301.123203 216.588206 \nL 301.423069 219.636882 \nL 301.722934 220.399051 \nL 302.0228 220.36418 \nL 302.322666 220.199791 \nL 302.622531 219.168621 \nL 302.922397 220.144994 \nL 303.522128 218.685416 \nL 303.821994 219.118806 \nL 304.12186 220.015475 \nL 304.421725 220.03042 \nL 305.021457 218.027859 \nL 305.321322 217.484876 \nL 305.621188 218.979324 \nL 305.921054 217.778784 \nL 306.220919 221.380405 \nL 306.520785 220.558458 \nL 307.120516 220.090198 \nL 307.420382 217.579524 \nL 307.720247 218.401471 \nL 308.319979 219.054047 \nL 308.619844 221.375424 \nL 309.219576 220.000531 \nL 309.519441 218.884676 \nL 309.819307 220.299421 \nL 310.119173 219.616956 \nL 310.419038 219.248325 \nL 310.718904 219.133751 \nL 311.01877 220.618236 \nL 311.318635 220.737792 \nL 311.618501 219.676734 \nL 311.918367 219.482455 \nL 312.218232 219.054047 \nL 312.518098 221.43022 \nL 312.817964 219.871012 \nL 313.417695 221.06159 \nL 313.71756 220.468792 \nL 314.017426 220.518607 \nL 314.317292 219.925809 \nL 314.617157 219.671752 \nL 314.917023 220.732811 \nL 315.216889 220.927089 \nL 315.516754 219.611974 \nL 315.81662 220.458829 \nL 316.116486 220.249606 \nL 316.416351 221.415276 \nL 316.716217 221.181145 \nL 317.016083 220.702922 \nL 317.315948 220.52857 \nL 317.615814 220.578384 \nL 317.91568 218.989287 \nL 318.215545 219.382825 \nL 318.515411 219.268251 \nL 318.815277 219.293159 \nL 319.115142 219.79629 \nL 319.415008 221.13133 \nL 320.014739 220.289458 \nL 320.314605 220.03042 \nL 320.914336 221.62948 \nL 321.214202 220.668051 \nL 321.514067 220.149976 \nL 321.813933 220.638162 \nL 322.113799 220.787607 \nL 322.413664 220.737792 \nL 322.71353 220.971923 \nL 323.013396 220.822478 \nL 323.313261 221.285757 \nL 323.613127 220.722848 \nL 323.912993 220.648125 \nL 324.212858 218.819916 \nL 324.512724 219.009213 \nL 325.112455 220.413995 \nL 325.712187 220.174883 \nL 326.012052 221.021738 \nL 326.311918 219.766401 \nL 326.611783 219.168621 \nL 326.911649 218.879694 \nL 327.211515 221.176164 \nL 327.51138 220.56344 \nL 327.811246 220.379125 \nL 328.111112 220.404032 \nL 328.710843 219.168621 \nL 329.010709 218.884676 \nL 329.310574 220.324328 \nL 329.910306 218.76512 \nL 330.210171 220.32931 \nL 330.510037 220.707903 \nL 330.809903 220.742774 \nL 331.109768 219.213455 \nL 331.409634 220.209754 \nL 331.7095 220.433921 \nL 332.009365 220.418977 \nL 332.309231 219.602011 \nL 332.609097 220.005512 \nL 332.908962 220.956978 \nL 333.208828 220.568421 \nL 333.508693 220.593329 \nL 333.808559 220.478755 \nL 334.108425 220.533551 \nL 334.40829 220.702922 \nL 334.708156 221.011775 \nL 335.008022 220.976904 \nL 335.307887 221.166201 \nL 335.607753 220.767681 \nL 335.907619 221.06159 \nL 336.207484 221.011775 \nL 336.807216 220.767681 \nL 337.107081 220.937052 \nL 337.406947 220.105142 \nL 337.706813 220.638162 \nL 338.006678 221.415276 \nL 338.306544 221.315646 \nL 338.60641 220.812515 \nL 338.906275 221.001812 \nL 339.206141 221.016756 \nL 339.506006 219.517326 \nL 339.805872 219.925809 \nL 340.105738 221.29572 \nL 340.405603 220.59831 \nL 340.705469 220.877274 \nL 341.3052 219.447585 \nL 341.605066 221.380405 \nL 341.904932 220.847385 \nL 342.204797 220.877274 \nL 342.504663 221.29572 \nL 342.804529 220.673033 \nL 343.104394 220.518607 \nL 343.40426 220.194809 \nL 343.704126 220.832441 \nL 344.003991 220.66307 \nL 344.303857 221.674314 \nL 344.603723 221.156238 \nL 344.903588 220.199791 \nL 345.203454 220.513625 \nL 345.50332 221.001812 \nL 345.803185 220.443884 \nL 346.402916 221.250886 \nL 346.702782 220.653107 \nL 347.002648 220.682996 \nL 347.302513 220.887237 \nL 347.602379 221.380405 \nL 347.902245 220.543514 \nL 348.20211 219.303122 \nL 348.501976 219.502381 \nL 348.801842 220.588347 \nL 349.101707 220.548495 \nL 349.401573 220.673033 \nL 349.701439 221.191108 \nL 350.001304 220.633181 \nL 350.30117 220.374143 \nL 350.601036 220.603292 \nL 351.200767 220.22968 \nL 351.500633 221.689258 \nL 351.800498 221.345535 \nL 352.100364 221.445165 \nL 352.40023 220.593329 \nL 352.700095 221.29572 \nL 352.999961 219.866031 \nL 353.299826 221.33059 \nL 353.599692 220.942034 \nL 354.199423 220.508644 \nL 354.499289 221.151256 \nL 354.799155 220.404032 \nL 355.09902 221.250886 \nL 355.398886 221.455128 \nL 355.698752 220.418977 \nL 355.998617 220.379125 \nL 356.298483 219.43264 \nL 356.598349 220.608273 \nL 356.898214 220.702922 \nL 357.19808 220.249606 \nL 357.497946 221.026719 \nL 357.797811 220.174883 \nL 358.097677 220.966941 \nL 358.397543 220.448866 \nL 358.697408 220.16492 \nL 358.997274 220.443884 \nL 359.297139 220.26455 \nL 359.597005 219.278214 \nL 359.896871 220.015475 \nL 360.196736 220.010494 \nL 360.496602 220.413995 \nL 360.796468 221.126349 \nL 361.096333 219.487437 \nL 361.396199 221.440183 \nL 361.696065 221.599591 \nL 361.99593 220.568421 \nL 362.595662 221.325609 \nL 363.195393 220.603292 \nL 363.495259 220.832441 \nL 363.795124 220.628199 \nL 364.09499 221.211034 \nL 364.394856 221.33059 \nL 364.694721 220.52857 \nL 364.994587 220.448866 \nL 365.294453 221.465091 \nL 365.594318 221.739073 \nL 365.594318 221.739073 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 46.0125 226.231635 \nL 46.0125 127.395271 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 380.8125 226.231635 \nL 380.8125 127.395271 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 46.0125 226.231635 \nL 380.8125 226.231635 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 46.0125 127.395271 \nL 380.8125 127.395271 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"peb99304ffb\">\n   <rect height=\"98.836364\" width=\"334.8\" x=\"46.0125\" y=\"8.791635\"/>\n  </clipPath>\n  <clipPath id=\"pe57e79cc53\">\n   <rect height=\"98.836364\" width=\"334.8\" x=\"46.0125\" y=\"127.395271\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Conv2d(input_channels, output_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1, 64, 1) \n",
    "        self.conv2 = nn.Conv2d(64, 128, 7)  \n",
    "        self.conv3 = nn.Conv2d(128, 128, 4)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fcOut = nn.Linear(4096, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def convs(self, x):\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        #1, 105, 105\n",
    "        print(x.dtype)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 64, 96, 96\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # 64, 48, 48\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        # 128, 42, 42\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # 128, 21, 21\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # 128, 18, 18\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # 128, 9, 9\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        # 256, 6, 6\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.type(torch.DoubleTensor)\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256 * 6 * 6)\n",
    "        x1 = self.sigmoid(self.fc1(x1))\n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256 * 6 * 6)\n",
    "        x2 = self.sigmoid(self.fc1(x2))\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fcOut(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "\n",
    "#creating the original network and couting the paramenters of different networks\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "siameseBaseLine = Net()\n",
    "siameseBaseLine = siameseBaseLine.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model architecture:\\n\\n', model)\n",
    "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
    "    \n",
    "count_parameters(siameseBaseLine)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model architecture:\n",
      "\n",
      " Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (fcOut): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "The model has 38,946,561 trainable parameters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "# training and validation after every epoch\n",
    "def train(model, train_loader, val_loader, num_epochs, criterion, save_name):\n",
    "    best_val_loss = float(\"Inf\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    cur_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        print(\"Starting epoch \" + str(epoch+1))\n",
    "        for img1, img2, labels in train_loader:\n",
    "            \n",
    "            # Forward\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(img1, img2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for img1, img2, labels in val_loader:\n",
    "                img1 = img1.to(device)\n",
    "                img2 = img2.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(img1, img2)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
    "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "    \n",
    "    print(\"Finished Training\")  \n",
    "    return train_losses, val_losses  \n",
    "\n",
    "# evaluation metrics\n",
    "def eval(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        print('Starting Iteration')\n",
    "        count = 0\n",
    "        for mainImg, imgSets, label in test_loader:\n",
    "            mainImg = mainImg.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            for i, testImg in enumerate(imgSets):\n",
    "                testImg = testImg.to(device)\n",
    "                output = model(mainImg, testImg)\n",
    "                if output > predVal:\n",
    "                    pred = i\n",
    "                    predVal = output\n",
    "            label = label.to(device)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "            if count % 20 == 0:\n",
    "                print(\"Current Count is: {}\".format(count))\n",
    "                print('Accuracy on n way: {}'.format(correct/count))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "# actual trainingd\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.0006)\n",
    "num_epochs = 50\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "save_path = 'siameseNet-batchnorm50.pt'\n",
    "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting epoch 1\n",
      "torch.float64\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6517/1049396224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'siameseNet-batchnorm50.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiameseBaseLine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6517/1543294907.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, criterion, save_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6517/1200036525.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6517/1200036525.py\u001b[0m in \u001b[0;36mconvs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#1, 105, 105\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# 64, 96, 96\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('p1_ml': conda)"
  },
  "interpreter": {
   "hash": "45ad390018fe16e49d36e7f3f69bb2868b1f449ecdbcbe075a23198a68f4bc93"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}