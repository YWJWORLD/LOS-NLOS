{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# imports\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision\n",
    "import torch.utils.data as utils\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils\n",
    "import time\n",
    "import copy\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## hyper-parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "random_seed = 42"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and Loading dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "with open('dataset.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df_uwb_data = data['data_CIR']\n",
    "df_uwb = data['data_t']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "## Train & valdiation & Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_uwb_data.values, df_uwb['NLOS'].values, test_size=0.1, random_state=random_seed, stratify=df_uwb['NLOS'].values)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=random_seed, stratify=y_train)\n",
    "\n",
    "print('X_train_shape {}, y_train_shape {} : '.format(X_train.shape, y_train.shape))\n",
    "# print('X_val_shape {}, y_val_shape {} : '.format(X_val.shape, y_val.shape))\n",
    "print('X_test_shape {}, y_test_shape {} : '.format(X_test.shape, y_test.shape))\n",
    "\n",
    "print(\"Train NLOS 0 count :\", len(y_train[y_train==0])) \n",
    "print(\"Train NLOS 1 count :\", len(y_train[y_train==1])) \n",
    "# print(\"Validation NLOS 0 count :\", len(y_val[y_val==0])) \n",
    "# print(\"Validation NLOS 1 count :\", len(y_val[y_val==1])) \n",
    "print(\"Test NLOS 0 count :\", len(y_test[y_test==0])) \n",
    "print(\"Test NLOS 0 count :\", len(y_test[y_test==1]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train_shape (37800, 1016), y_train_shape (37800,) : \n",
      "X_test_shape (4200, 1016), y_test_shape (4200,) : \n",
      "Train NLOS 0 count : 18900\n",
      "Train NLOS 1 count : 18900\n",
      "Test NLOS 0 count : 2100\n",
      "Test NLOS 0 count : 2100\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Amplitude to dB Scale"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "## amplitude to dB\n",
    "X_train_dB = librosa.amplitude_to_db(X_train)\n",
    "X_test_dB = librosa.amplitude_to_db(X_test)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X_train[0])\n",
    "# plt.figure()\n",
    "# plt.plot(X_train[1])\n",
    "# plt.figure()\n",
    "# plt.plot(X_train_dB[0])\n",
    "\n",
    "## nomalization\n",
    "# scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "# X_train_SC = scaler.transform(X_train)\n",
    "# X_test_SC = scaler.transform(X_test)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(X_train_SC[0])\n",
    "# plt.figure()\n",
    "# plt.plot(X_train_SC[1])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "idx_los = np.where(y_train==0)[0]\n",
    "idx_nlos = np.where(y_train==1)[0]\n",
    "print(idx_los)\n",
    "print(idx_nlos)\n",
    "X_train_vec = X_train.reshape(-1,1,1016)\n",
    "X_test_vec = X_test.reshape(-1,1,1016)\n",
    "# X_test_vec.shape\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[    7     8     9 ... 37797 37798 37799]\n",
      "[    0     1     2 ... 37787 37791 37796]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "class SiameseNetworkDataset():\n",
    "    \n",
    "    def __init__(self,training_data=None,training_dir=None, setSize=None, transform=None):\n",
    "        # used to prepare the labels and images path\n",
    "        self.training_data = training_data\n",
    "        # self.training_df.columns =[\"image1\",\"image2\",\"label\"]\n",
    "        self.training_dir = training_dir    \n",
    "        self.transform = transform\n",
    "        self.setSize = setSize\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    def __getitem__(self,idx):\n",
    "        # get class index\n",
    "        idx_los = np.where(self.training_dir==0)[0]\n",
    "        idx_nlos = np.where(self.training_dir==1)[0]\n",
    "        # select label\n",
    "        if idx % 2 == 0:\n",
    "            category = random.randint(0,1)\n",
    "            if category == 0:\n",
    "                img1_idx = random.choice(idx_los) # LOS\n",
    "                img2_idx = random.choice(idx_los)\n",
    "                \n",
    "            else:\n",
    "                img1_idx = random.choice(idx_nlos) # NLOS\n",
    "                img2_idx = random.choice(idx_nlos)\n",
    "            img1 = self.training_data[img1_idx] \n",
    "            img2 = self.training_data[img2_idx]\n",
    "            label = 1.0\n",
    "\n",
    "        else:\n",
    "            category2 = random.randint(0,1)\n",
    "            if category2 == 0:\n",
    "                img1_idx = random.choice(idx_los)\n",
    "                img2_idx = random.choice(idx_nlos)\n",
    "            else:\n",
    "                img1_idx = random.choice(idx_nlos)\n",
    "                img2_idx = random.choice(idx_los)\n",
    "            img1 = self.training_data[img1_idx] \n",
    "            img2 = self.training_data[img2_idx]\n",
    "            label = 0.0\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.from_numpy(np.array([label], dtype=np.float32))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "\n",
    "# creates n-way one shot learning evaluation\n",
    "class SiameseEvalSet(Dataset):\n",
    "    def __init__(self, test_data, test_dir, setSize, numWay, transform=None):\n",
    "        self.test_data = test_data\n",
    "        # self.test_df.columns =[\"image1\",\"image2\",\"label\"]\n",
    "        self.test_dir = test_dir    \n",
    "        self.transform = transform\n",
    "        self.setSize = setSize\n",
    "    def __len__(self):\n",
    "        return self.setSize\n",
    "    def __getitem__(self, idx):\n",
    "        idx_los = np.where(self.test_dir==0)[0]\n",
    "        idx_nlos = np.where(self.test_dir==1)[0]\n",
    "        # find one main image, 0 is LOS , 1 is NLOS\n",
    "        category = random.randint(0,1)\n",
    "        if category == 0: # if selected label is LOS\n",
    "            mainimg_idx = random.choice(idx_los) # LOS\n",
    "        else:\n",
    "            mainimg_idx = random.choice(idx_nlos) \n",
    "\n",
    "        mainimg = test_data[mainimg_idx]\n",
    "        if self.transfrom:\n",
    "            mainImg = self.transfrom(mainimg)\n",
    "\n",
    "        # find n numbers of distinct images, 1 in the same set as the main\n",
    "        testSet = []\n",
    "        label = np.random.randint(self.numWay) # 0 or 1\n",
    "        for i in range(self.numWay):\n",
    "            if i == label:\n",
    "                if category == 0:\n",
    "                    test_idx = random_choice(idx_los)\n",
    "                else:\n",
    "                    test_idx = random.choice(idx_nlos)\n",
    "            else:\n",
    "                if category == 0:\n",
    "                    test_idx = random.choice(idx_nlos)\n",
    "                else:\n",
    "                    test_idx = random.choice(idx_los)\n",
    "\n",
    "            testImg = test_data[test_idx]\n",
    "            if self.transform:\n",
    "                testImg = self.transform(testImg)\n",
    "            testSet.append(testImg)\n",
    "            # plt.imshow()\n",
    "            return mainImg, testSet, torch.from_numpy(np.array([label], dtype = int))\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# choose a training dataset size and further divide it into train and validation set 80:20\n",
    "dataSize = 100000 # self-defined dataset size\n",
    "TRAIN_PCT = 0.8 # percentage of entire dataset for training\n",
    "train_size = int(dataSize * TRAIN_PCT)\n",
    "val_size = dataSize - train_size\n",
    "print(train_size,val_size)\n",
    "transformations = transforms.Compose(\n",
    "    [transforms.ToTensor()]) \n",
    "\n",
    "CIRdataset = SiameseNetworkDataset(training_data = X_train_vec, training_dir=y_train, setSize=dataSize, transform=transformations)\n",
    "train_set, val_set = random_split(CIRdataset, [train_size, val_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, num_workers=2, shuffle=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80000 20000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# Viewing the sample of images and to check whetehr its loading properly\n",
    "dataloader = DataLoader(CIRdataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=1)\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "image1, image2, label = dataiter.next()\n",
    "\n",
    "print(label)\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(image1.squeeze())\n",
    "plt.subplot(212)\n",
    "plt.plot(image2.squeeze())\n",
    "\n",
    "# concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "# imshow(torchvision.utils.make_grid(concatenated))\n",
    "# print(example_batch[2].numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1.]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4d180dad00>]"
      ]
     },
     "metadata": {},
     "execution_count": 68
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 388.0125 248.518125\" width=\"388.0125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-09-03T17:54:49.879950</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 388.0125 248.518125 \nL 388.0125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.0125 106.036364 \nL 380.8125 106.036364 \nL 380.8125 7.2 \nL 46.0125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m41c3de3f4b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#m41c3de3f4b\" y=\"106.036364\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(58.049432 120.634801)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.203812\" xlink:href=\"#m41c3de3f4b\" y=\"106.036364\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(111.660062 120.634801)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.176942\" xlink:href=\"#m41c3de3f4b\" y=\"106.036364\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(171.633192 120.634801)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.150073\" xlink:href=\"#m41c3de3f4b\" y=\"106.036364\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(231.606323 120.634801)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.123203\" xlink:href=\"#m41c3de3f4b\" y=\"106.036364\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <g transform=\"translate(291.579453 120.634801)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.096333\" xlink:href=\"#m41c3de3f4b\" y=\"106.036364\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <g transform=\"translate(348.371333 120.634801)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mac29422a27\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#mac29422a27\" y=\"101.543802\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(32.65 105.34302)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#mac29422a27\" y=\"75.717058\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5000 -->\n      <g transform=\"translate(13.5625 79.516277)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#mac29422a27\" y=\"49.890315\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10000 -->\n      <g transform=\"translate(7.2 53.689534)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#mac29422a27\" y=\"24.063572\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15000 -->\n      <g transform=\"translate(7.2 27.862791)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#pc5a5ff81cf)\" d=\"M 61.230682 100.169819 \nL 61.530547 101.00144 \nL 61.830413 100.841314 \nL 62.130279 100.107835 \nL 62.430144 100.288622 \nL 62.73001 101.063424 \nL 63.029876 100.391929 \nL 63.329741 100.562385 \nL 63.629607 100.185315 \nL 63.929473 100.242134 \nL 64.229338 100.707015 \nL 64.529204 100.810322 \nL 64.82907 100.283457 \nL 65.128935 101.140904 \nL 65.428801 100.391929 \nL 66.028532 100.758669 \nL 66.328398 100.221472 \nL 66.628264 100.62437 \nL 66.928129 100.345441 \nL 67.227995 100.453913 \nL 67.527861 100.355771 \nL 67.827726 100.484905 \nL 68.127592 100.996275 \nL 68.427457 100.417756 \nL 68.727323 101.130574 \nL 69.027189 101.130574 \nL 69.327054 100.433252 \nL 69.926786 100.355771 \nL 70.226651 100.076843 \nL 70.526517 100.185315 \nL 70.826383 100.009693 \nL 71.126248 100.645031 \nL 71.426114 100.407425 \nL 71.72598 100.794826 \nL 72.025845 100.433252 \nL 72.325711 100.603708 \nL 72.625577 100.397094 \nL 73.225308 100.929125 \nL 73.525174 100.748338 \nL 73.825039 100.087173 \nL 74.124905 100.164654 \nL 74.42477 101.058259 \nL 74.724636 100.159488 \nL 75.024502 100.459078 \nL 75.324367 99.064434 \nL 75.924099 100.92396 \nL 76.223964 100.004528 \nL 76.52383 100.33511 \nL 77.423427 100.655362 \nL 77.723293 100.567551 \nL 78.023158 100.304118 \nL 78.323024 100.443582 \nL 78.62289 101.182227 \nL 78.922755 100.696684 \nL 79.522487 100.851645 \nL 79.822352 100.583047 \nL 80.122218 100.913629 \nL 80.721949 100.133661 \nL 81.021815 101.037597 \nL 81.621546 99.983866 \nL 81.921412 99.870229 \nL 82.221277 100.309283 \nL 82.821009 100.087173 \nL 83.120874 100.670858 \nL 83.42074 100.288622 \nL 83.720606 101.016936 \nL 84.320337 100.47974 \nL 84.620203 100.670858 \nL 84.920068 99.854733 \nL 85.219934 100.608874 \nL 85.5198 100.655362 \nL 85.819665 100.438417 \nL 86.119531 100.004528 \nL 86.419397 100.562385 \nL 86.719262 100.469409 \nL 87.019128 100.805157 \nL 87.318994 100.965283 \nL 87.618859 100.686354 \nL 87.918725 100.159488 \nL 88.21859 100.505567 \nL 88.518456 100.056181 \nL 89.118187 101.058259 \nL 89.418053 100.329945 \nL 89.717919 100.960117 \nL 90.617516 100.681188 \nL 90.917381 99.916717 \nL 91.517113 100.552055 \nL 91.816978 100.598543 \nL 92.116844 100.949787 \nL 92.716575 100.784495 \nL 93.016441 100.247299 \nL 93.316307 100.763834 \nL 93.616172 100.211142 \nL 93.916038 101.00144 \nL 94.215903 100.231803 \nL 94.515769 100.350606 \nL 94.815635 100.603708 \nL 95.1155 101.202889 \nL 95.415366 98.85782 \nL 95.715232 100.608874 \nL 96.015097 100.252464 \nL 96.314963 100.650196 \nL 96.614829 100.025189 \nL 96.914694 100.195646 \nL 97.21456 101.109912 \nL 97.514426 100.505567 \nL 97.814291 100.391929 \nL 98.114157 100.113 \nL 98.713888 100.810322 \nL 99.013754 100.33511 \nL 99.31362 100.851645 \nL 99.613485 99.797914 \nL 99.913351 100.867141 \nL 100.513082 100.195646 \nL 100.812948 100.474574 \nL 101.112813 101.151235 \nL 101.412679 100.288622 \nL 101.712545 100.975613 \nL 102.01241 101.037597 \nL 102.312276 100.851645 \nL 102.612142 100.211142 \nL 102.912007 100.340275 \nL 103.211873 100.913629 \nL 103.511739 100.107835 \nL 104.411336 100.985944 \nL 104.711201 101.352684 \nL 105.011067 101.233881 \nL 105.310933 100.366102 \nL 105.610798 100.438417 \nL 106.21053 100.174984 \nL 106.810261 100.799991 \nL 107.110127 100.758669 \nL 107.409992 100.572716 \nL 107.709858 100.154323 \nL 108.009723 101.30103 \nL 108.309589 101.321692 \nL 108.609455 100.345441 \nL 108.90932 100.433252 \nL 109.209186 101.306196 \nL 109.509052 101.047928 \nL 109.808917 100.474574 \nL 110.108783 100.33511 \nL 110.408649 100.33511 \nL 110.708514 101.414668 \nL 111.00838 100.061347 \nL 111.308246 100.200811 \nL 111.907977 101.182227 \nL 112.207843 101.321692 \nL 112.507708 100.882637 \nL 112.807574 101.270038 \nL 113.10744 101.166731 \nL 113.407305 100.195646 \nL 113.707171 100.784495 \nL 114.007036 101.07892 \nL 114.606768 99.839237 \nL 114.906633 99.756591 \nL 115.206499 101.171897 \nL 115.506365 100.515897 \nL 115.80623 100.872306 \nL 116.106096 100.226638 \nL 116.405962 100.221472 \nL 116.705827 100.583047 \nL 117.005693 100.293787 \nL 117.605424 100.805157 \nL 117.90529 99.844402 \nL 118.505021 100.799991 \nL 118.804887 100.748338 \nL 119.104753 100.85681 \nL 119.704484 99.880559 \nL 120.00435 101.30103 \nL 120.304215 100.908464 \nL 120.604081 100.304118 \nL 120.903946 101.00144 \nL 121.203812 100.293787 \nL 121.803543 100.541724 \nL 122.103409 100.273126 \nL 122.403275 100.247299 \nL 122.70314 100.980779 \nL 123.003006 101.125408 \nL 123.302872 100.598543 \nL 123.602737 100.381598 \nL 123.902603 100.732842 \nL 124.202469 100.861976 \nL 124.502334 99.932213 \nL 124.8022 100.397094 \nL 125.102066 100.47974 \nL 125.401931 100.143992 \nL 125.701797 100.407425 \nL 126.001663 101.357849 \nL 126.301528 101.197723 \nL 126.90126 100.154323 \nL 127.500991 100.428086 \nL 127.800856 100.877472 \nL 128.100722 99.395017 \nL 128.400588 100.872306 \nL 129.000319 100.33511 \nL 129.300185 100.464244 \nL 129.60005 100.19048 \nL 129.899916 100.097504 \nL 130.199782 100.464244 \nL 130.799513 100.092339 \nL 131.099379 101.244211 \nL 131.399244 100.898133 \nL 131.69911 100.350606 \nL 131.998976 100.118165 \nL 132.298841 101.321692 \nL 132.598707 100.546889 \nL 132.898573 101.409503 \nL 133.198438 100.614039 \nL 133.498304 100.355771 \nL 133.79817 101.342353 \nL 134.098035 99.219395 \nL 134.397901 99.932213 \nL 134.697766 101.016936 \nL 134.997632 100.174984 \nL 135.297498 101.022101 \nL 135.597363 100.789661 \nL 135.897229 100.77933 \nL 136.197095 100.242134 \nL 136.49696 100.768999 \nL 136.796826 100.453913 \nL 137.096692 100.541724 \nL 137.396557 100.19048 \nL 137.996289 100.62437 \nL 138.296154 101.109912 \nL 138.59602 100.552055 \nL 138.895886 100.820653 \nL 139.195751 100.329945 \nL 139.495617 101.032432 \nL 139.795483 100.521063 \nL 140.095348 101.270038 \nL 140.395214 100.422921 \nL 140.695079 100.174984 \nL 141.294811 100.815487 \nL 141.594676 100.407425 \nL 141.894542 100.815487 \nL 142.494273 100.40226 \nL 142.794139 100.25763 \nL 143.094005 101.063424 \nL 143.39387 100.205976 \nL 143.693736 100.665692 \nL 143.993602 99.916717 \nL 144.293467 100.825818 \nL 144.593333 99.906386 \nL 144.893199 100.794826 \nL 145.193064 100.707015 \nL 145.49293 99.932213 \nL 145.792796 100.521063 \nL 146.092661 100.438417 \nL 146.392527 100.820653 \nL 146.692393 100.422921 \nL 146.992258 100.247299 \nL 147.292124 100.753503 \nL 147.591989 100.433252 \nL 147.891855 100.799991 \nL 148.191721 100.918794 \nL 148.791452 100.226638 \nL 149.091318 101.063424 \nL 149.391183 100.732842 \nL 149.691049 100.887802 \nL 149.990915 100.205976 \nL 150.29078 100.562385 \nL 150.590646 100.593378 \nL 150.890512 100.252464 \nL 151.190377 100.660527 \nL 151.490243 100.830984 \nL 151.790109 99.947709 \nL 152.089974 101.192558 \nL 152.38984 100.283457 \nL 152.689706 100.645031 \nL 152.989571 100.154323 \nL 153.289437 100.464244 \nL 153.889168 99.921882 \nL 154.488899 101.218385 \nL 154.788765 100.293787 \nL 155.088631 100.41259 \nL 155.988228 99.81341 \nL 156.587959 101.073755 \nL 156.887825 100.577881 \nL 157.18769 100.92396 \nL 157.487556 100.314449 \nL 157.787422 100.841314 \nL 158.087287 100.319614 \nL 158.687019 100.298953 \nL 158.986884 100.753503 \nL 159.28675 100.892968 \nL 159.886481 100.236968 \nL 160.186347 100.929125 \nL 160.486212 100.247299 \nL 160.786078 101.058259 \nL 161.085944 101.156401 \nL 161.385809 101.016936 \nL 161.685675 100.639866 \nL 161.985541 99.999362 \nL 162.585272 100.908464 \nL 162.885138 100.619204 \nL 163.185003 100.794826 \nL 163.484869 100.045851 \nL 163.784735 100.40226 \nL 164.0846 100.562385 \nL 164.384466 99.906386 \nL 164.684332 100.794826 \nL 164.984197 100.794826 \nL 165.284063 100.934291 \nL 165.583929 100.768999 \nL 165.883794 101.089251 \nL 166.18366 100.484905 \nL 166.783391 100.004528 \nL 167.383122 101.435329 \nL 167.682988 100.133661 \nL 167.982854 100.288622 \nL 168.282719 101.419833 \nL 168.882451 100.304118 \nL 169.182316 100.273126 \nL 169.482182 100.996275 \nL 169.782048 100.314449 \nL 170.081913 100.593378 \nL 170.381779 100.298953 \nL 170.681645 101.011771 \nL 170.98151 100.505567 \nL 171.281376 100.77933 \nL 171.581242 100.614039 \nL 171.881107 100.681188 \nL 172.180973 101.14607 \nL 172.480839 100.273126 \nL 172.780704 100.174984 \nL 173.08057 101.213219 \nL 173.380436 101.042763 \nL 173.680301 100.40226 \nL 173.980167 100.231803 \nL 174.280032 100.980779 \nL 174.579898 101.06859 \nL 174.879764 100.309283 \nL 175.179629 100.118165 \nL 175.479495 100.655362 \nL 175.779361 100.70185 \nL 176.079226 101.280369 \nL 176.379092 100.453913 \nL 176.678958 100.407425 \nL 176.978823 100.732842 \nL 177.278689 100.309283 \nL 177.578555 100.717346 \nL 177.87842 100.040685 \nL 178.178286 101.177062 \nL 178.778017 100.020024 \nL 179.077883 101.032432 \nL 179.377749 100.360937 \nL 179.677614 100.216307 \nL 179.97748 100.614039 \nL 180.277345 100.500401 \nL 180.577211 100.670858 \nL 180.877077 100.092339 \nL 181.476808 100.92396 \nL 181.776674 100.128496 \nL 182.376405 100.273126 \nL 182.676271 100.965283 \nL 182.976136 100.84648 \nL 183.276002 100.314449 \nL 183.575868 100.200811 \nL 184.175599 100.505567 \nL 184.475465 100.014858 \nL 184.77533 99.916717 \nL 185.075196 100.707015 \nL 185.375062 100.236968 \nL 185.674927 99.054104 \nL 185.974793 100.639866 \nL 186.574524 100.154323 \nL 186.87439 99.549977 \nL 187.474121 100.918794 \nL 187.773987 100.464244 \nL 188.073852 101.239046 \nL 188.373718 100.391929 \nL 188.673584 100.19048 \nL 188.973449 100.293787 \nL 189.273315 101.187393 \nL 189.873046 99.906386 \nL 190.172912 100.526228 \nL 190.472778 100.608874 \nL 190.772643 100.366102 \nL 191.072509 100.712181 \nL 191.372375 100.226638 \nL 191.67224 100.113 \nL 191.972106 101.466321 \nL 192.271972 100.453913 \nL 192.571837 100.087173 \nL 192.871703 99.932213 \nL 193.171569 100.113 \nL 193.471434 100.949787 \nL 194.071165 100.33511 \nL 194.371031 100.495236 \nL 194.670897 101.032432 \nL 194.970762 100.85681 \nL 195.270628 100.061347 \nL 195.570494 100.267961 \nL 195.870359 100.872306 \nL 196.170225 100.41259 \nL 196.470091 100.686354 \nL 196.769956 100.014858 \nL 197.369688 101.135739 \nL 197.669553 100.484905 \nL 197.969419 100.722511 \nL 198.269285 101.270038 \nL 198.56915 100.164654 \nL 199.168882 100.03552 \nL 199.468747 100.944621 \nL 199.768613 100.143992 \nL 200.068479 100.960117 \nL 200.368344 100.221472 \nL 200.66821 100.484905 \nL 200.968075 100.376433 \nL 201.267941 100.448748 \nL 201.567807 100.376433 \nL 201.867672 100.459078 \nL 202.167538 100.309283 \nL 202.767269 100.799991 \nL 203.067135 101.399172 \nL 203.367001 100.329945 \nL 203.666866 100.836149 \nL 203.966732 100.185315 \nL 204.266598 100.898133 \nL 204.566463 100.996275 \nL 204.866329 99.219395 \nL 205.166195 100.583047 \nL 205.46606 101.285534 \nL 205.765926 100.020024 \nL 206.365657 101.125408 \nL 207.265254 99.911551 \nL 207.56512 100.712181 \nL 207.864985 100.965283 \nL 208.164851 100.438417 \nL 208.464717 100.505567 \nL 208.764582 99.844402 \nL 209.064448 101.120243 \nL 209.364314 100.77933 \nL 209.664179 101.202889 \nL 209.964045 100.386764 \nL 210.563776 101.047928 \nL 210.863642 100.071677 \nL 211.463373 100.355771 \nL 211.763239 100.918794 \nL 212.063105 100.732842 \nL 212.36297 100.758669 \nL 212.662836 100.422921 \nL 212.962702 100.360937 \nL 213.262567 101.063424 \nL 214.162164 100.149158 \nL 214.46203 100.304118 \nL 214.761895 100.009693 \nL 215.361627 100.128496 \nL 215.661492 100.47974 \nL 215.961358 100.655362 \nL 216.261224 100.965283 \nL 216.561089 100.087173 \nL 216.860955 100.743173 \nL 217.160821 99.95804 \nL 217.460686 100.639866 \nL 218.060418 100.283457 \nL 218.360283 100.882637 \nL 218.660149 100.376433 \nL 218.960015 100.174984 \nL 219.25988 100.743173 \nL 219.559746 99.937378 \nL 219.859612 100.934291 \nL 220.159477 101.06859 \nL 220.459343 100.009693 \nL 220.759208 100.84648 \nL 221.059074 100.102669 \nL 221.35894 100.861976 \nL 221.658805 100.247299 \nL 222.258537 100.273126 \nL 222.858268 100.577881 \nL 223.158134 100.169819 \nL 223.457999 101.084086 \nL 224.057731 100.123331 \nL 224.357596 100.216307 \nL 225.257193 101.44566 \nL 225.557059 100.438417 \nL 225.856925 100.490071 \nL 226.15679 100.097504 \nL 226.456656 100.376433 \nL 226.756521 101.058259 \nL 227.056387 100.908464 \nL 227.356253 100.438417 \nL 227.656118 100.619204 \nL 227.955984 101.321692 \nL 228.25585 100.552055 \nL 228.555715 100.433252 \nL 228.855581 99.906386 \nL 229.155447 100.541724 \nL 229.455312 100.247299 \nL 229.755178 100.386764 \nL 230.055044 100.143992 \nL 230.354909 100.169819 \nL 230.654775 99.787583 \nL 230.954641 100.851645 \nL 231.554372 100.727677 \nL 231.854238 100.417756 \nL 232.453969 100.464244 \nL 232.753835 100.056181 \nL 233.0537 100.288622 \nL 233.353566 101.130574 \nL 233.653431 100.841314 \nL 233.953297 99.679111 \nL 234.253163 100.164654 \nL 234.553028 100.309283 \nL 234.852894 100.753503 \nL 235.15276 100.113 \nL 235.452625 100.128496 \nL 235.752491 98.852655 \nL 236.052357 100.996275 \nL 236.352222 101.00144 \nL 236.652088 100.267961 \nL 236.951954 100.433252 \nL 237.251819 99.963205 \nL 237.551685 101.104747 \nL 237.851551 100.639866 \nL 238.151416 101.022101 \nL 238.451282 100.350606 \nL 238.751148 100.211142 \nL 239.051013 100.676023 \nL 239.350879 100.319614 \nL 239.650745 100.892968 \nL 239.95061 100.500401 \nL 240.250476 99.544812 \nL 240.550341 100.226638 \nL 240.850207 99.81341 \nL 241.150073 100.588212 \nL 241.449938 100.55722 \nL 241.749804 100.273126 \nL 242.349535 100.851645 \nL 242.649401 100.185315 \nL 242.949267 100.490071 \nL 243.249132 100.407425 \nL 243.548998 100.521063 \nL 243.848864 100.25763 \nL 244.148729 100.267961 \nL 244.448595 100.438417 \nL 244.748461 100.174984 \nL 245.048326 100.727677 \nL 245.348192 99.947709 \nL 245.648058 99.601631 \nL 245.947923 100.453913 \nL 246.547655 99.906386 \nL 246.84752 100.877472 \nL 247.147386 100.546889 \nL 247.447251 100.62437 \nL 247.747117 101.063424 \nL 248.046983 100.639866 \nL 248.346848 100.562385 \nL 248.94658 100.913629 \nL 249.246445 100.262795 \nL 249.846177 101.239046 \nL 250.445908 100.123331 \nL 250.745774 100.355771 \nL 251.045639 100.41259 \nL 251.345505 99.865063 \nL 251.645371 99.927048 \nL 251.945236 100.794826 \nL 252.245102 100.345441 \nL 252.544968 100.650196 \nL 252.844833 100.247299 \nL 253.444564 100.577881 \nL 253.74443 100.355771 \nL 254.044296 100.536559 \nL 254.344161 100.298953 \nL 254.644027 100.562385 \nL 254.943893 100.975613 \nL 255.243758 100.552055 \nL 255.543624 100.619204 \nL 256.143355 100.267961 \nL 256.443221 100.428086 \nL 256.743087 100.722511 \nL 257.042952 100.366102 \nL 257.642684 101.099582 \nL 257.942549 100.18015 \nL 258.242415 99.870229 \nL 258.542281 100.448748 \nL 258.842146 100.6347 \nL 259.142012 99.906386 \nL 259.441878 100.861976 \nL 259.741743 100.309283 \nL 260.041609 100.521063 \nL 260.341474 100.562385 \nL 260.64134 100.85681 \nL 260.941206 99.952874 \nL 261.840803 101.047928 \nL 262.140668 100.267961 \nL 262.440534 100.097504 \nL 262.7404 100.304118 \nL 263.040265 100.004528 \nL 263.340131 100.092339 \nL 263.939862 100.670858 \nL 264.239728 100.366102 \nL 264.539594 101.352684 \nL 264.839459 99.436339 \nL 265.139325 100.515897 \nL 265.439191 99.859898 \nL 266.038922 100.515897 \nL 266.338788 99.927048 \nL 266.638653 100.996275 \nL 266.938519 100.887802 \nL 267.238384 100.603708 \nL 267.53825 101.135739 \nL 268.137981 100.164654 \nL 268.437847 100.84648 \nL 268.737713 100.650196 \nL 269.037578 100.211142 \nL 269.63731 100.267961 \nL 269.937175 100.118165 \nL 270.237041 100.650196 \nL 270.836772 99.906386 \nL 271.436504 100.918794 \nL 271.736369 100.939456 \nL 272.036235 101.450825 \nL 272.336101 101.156401 \nL 272.635966 101.270038 \nL 273.235697 100.242134 \nL 273.535563 101.244211 \nL 273.835429 101.275204 \nL 274.135294 100.841314 \nL 274.43516 100.861976 \nL 275.034891 100.045851 \nL 275.634623 100.77933 \nL 275.934488 101.233881 \nL 276.234354 100.593378 \nL 276.53422 100.670858 \nL 276.834085 101.202889 \nL 277.133951 100.138827 \nL 277.433817 100.763834 \nL 277.733682 100.014858 \nL 278.033548 100.185315 \nL 278.333414 99.89089 \nL 278.633279 100.40226 \nL 278.933145 100.40226 \nL 279.233011 100.934291 \nL 279.532876 100.03552 \nL 279.832742 100.376433 \nL 280.132607 100.443582 \nL 280.432473 100.707015 \nL 280.732339 100.113 \nL 281.032204 100.660527 \nL 281.33207 100.70185 \nL 281.631936 101.228715 \nL 281.931801 100.262795 \nL 282.231667 100.113 \nL 282.531533 99.379521 \nL 282.831398 100.329945 \nL 283.131264 100.521063 \nL 283.43113 100.154323 \nL 283.730995 100.484905 \nL 284.030861 100.273126 \nL 284.330727 100.84648 \nL 284.630592 100.629535 \nL 284.930458 101.037597 \nL 285.230324 99.694607 \nL 285.530189 100.283457 \nL 285.830055 87.814305 \nL 286.429786 11.692562 \nL 286.729652 14.373378 \nL 287.029517 43.970826 \nL 287.329383 44.415046 \nL 287.629249 54.131066 \nL 287.929114 55.975096 \nL 288.22898 79.880329 \nL 288.828711 97.308216 \nL 289.128577 88.268856 \nL 289.428443 95.505509 \nL 289.728308 86.497141 \nL 290.32804 53.929618 \nL 290.627905 76.218097 \nL 290.927771 83.37727 \nL 291.227637 60.871846 \nL 291.527502 60.9235 \nL 292.127234 82.473334 \nL 292.427099 85.500229 \nL 292.726965 94.963147 \nL 293.02683 95.107777 \nL 293.326696 95.706958 \nL 293.626562 92.364977 \nL 293.926427 90.77405 \nL 294.226293 101.321692 \nL 294.526159 95.954894 \nL 294.826024 96.755523 \nL 295.12589 99.570638 \nL 295.425756 95.706958 \nL 295.725621 98.666702 \nL 296.025487 98.542734 \nL 296.325353 97.953884 \nL 296.625218 94.6274 \nL 296.925084 94.074708 \nL 297.22495 95.799934 \nL 297.524815 95.867083 \nL 297.824681 92.375308 \nL 298.124547 95.097447 \nL 298.424412 99.963205 \nL 298.724278 95.975556 \nL 299.324009 98.842324 \nL 299.623875 98.661537 \nL 299.92374 98.145002 \nL 300.223606 95.965225 \nL 300.523472 91.931088 \nL 300.823337 95.061289 \nL 301.123203 91.96208 \nL 301.423069 87.58703 \nL 301.722934 91.161451 \nL 302.322666 99.859898 \nL 302.622531 96.492091 \nL 302.922397 98.103679 \nL 303.222263 95.37121 \nL 303.522128 97.308216 \nL 303.821994 101.011771 \nL 304.421725 97.117098 \nL 304.721591 98.708025 \nL 305.021457 95.634643 \nL 305.321322 90.99616 \nL 305.621188 93.051968 \nL 305.921054 95.975556 \nL 306.520785 95.29373 \nL 307.120516 100.84648 \nL 307.420382 97.793759 \nL 308.020113 100.128496 \nL 308.319979 97.571649 \nL 308.91971 99.028277 \nL 309.219576 97.602641 \nL 309.519441 98.899143 \nL 309.819307 99.44667 \nL 310.119173 100.484905 \nL 310.419038 100.205976 \nL 310.718904 99.627457 \nL 311.01877 100.686354 \nL 311.318635 97.416688 \nL 311.618501 97.3702 \nL 311.918367 98.40327 \nL 312.218232 98.160498 \nL 312.518098 100.133661 \nL 312.817964 98.145002 \nL 313.117829 98.186325 \nL 313.417695 100.546889 \nL 313.71756 100.484905 \nL 314.017426 99.090261 \nL 314.317292 99.178072 \nL 314.617157 99.596465 \nL 314.917023 100.763834 \nL 315.216889 99.818575 \nL 315.516754 99.353694 \nL 315.81662 100.867141 \nL 316.116486 100.009693 \nL 316.416351 99.756591 \nL 316.716217 100.918794 \nL 317.016083 99.085096 \nL 317.315948 99.808245 \nL 317.91568 100.340275 \nL 318.215545 100.025189 \nL 318.515411 99.482828 \nL 318.815277 100.014858 \nL 319.115142 99.30204 \nL 319.415008 101.30103 \nL 320.014739 99.240056 \nL 320.61447 99.353694 \nL 320.914336 100.345441 \nL 321.214202 100.252464 \nL 321.514067 100.521063 \nL 322.113799 99.854733 \nL 322.71353 100.40226 \nL 323.313261 99.296875 \nL 323.613127 100.85681 \nL 323.912993 100.014858 \nL 324.212858 99.901221 \nL 324.512724 99.131584 \nL 324.81259 100.211142 \nL 325.112455 100.25763 \nL 325.412321 100.082008 \nL 325.712187 100.407425 \nL 326.012052 99.963205 \nL 326.311918 101.22355 \nL 326.611783 100.205976 \nL 326.911649 100.799991 \nL 327.211515 100.593378 \nL 327.51138 100.583047 \nL 327.811246 100.722511 \nL 328.410977 100.226638 \nL 328.710843 100.345441 \nL 329.61044 101.073755 \nL 329.910306 100.216307 \nL 330.210171 100.743173 \nL 330.510037 99.978701 \nL 331.109768 100.918794 \nL 331.409634 100.154323 \nL 331.7095 101.120243 \nL 332.009365 100.329945 \nL 332.309231 98.997285 \nL 332.609097 99.260718 \nL 332.908962 100.407425 \nL 333.208828 100.872306 \nL 333.508693 99.978701 \nL 333.808559 100.991109 \nL 334.40829 100.025189 \nL 335.907619 100.608874 \nL 336.207484 100.464244 \nL 336.50735 100.583047 \nL 336.807216 101.115078 \nL 337.107081 99.260718 \nL 337.406947 100.608874 \nL 337.706813 100.593378 \nL 338.006678 100.92396 \nL 338.306544 100.386764 \nL 338.906275 100.774165 \nL 339.506006 100.55722 \nL 339.805872 99.756591 \nL 340.105738 100.252464 \nL 340.705469 100.226638 \nL 341.005335 100.500401 \nL 341.3052 100.469409 \nL 341.605066 100.143992 \nL 341.904932 100.123331 \nL 342.204797 100.236968 \nL 342.504663 101.084086 \nL 342.804529 100.159488 \nL 343.104394 100.459078 \nL 343.40426 100.252464 \nL 343.704126 100.92396 \nL 344.003991 100.851645 \nL 344.603723 100.252464 \nL 344.903588 100.397094 \nL 345.203454 101.00144 \nL 345.50332 100.588212 \nL 345.803185 100.438417 \nL 346.103051 99.477662 \nL 346.402916 100.598543 \nL 346.702782 100.092339 \nL 347.002648 100.18015 \nL 347.302513 100.650196 \nL 347.602379 100.03552 \nL 347.902245 99.911551 \nL 348.20211 100.691519 \nL 348.501976 100.226638 \nL 348.801842 100.536559 \nL 349.101707 100.329945 \nL 349.401573 100.298953 \nL 349.701439 100.474574 \nL 350.001304 101.44566 \nL 350.30117 101.161566 \nL 350.601036 100.273126 \nL 350.900901 100.040685 \nL 351.200767 100.753503 \nL 351.500633 100.092339 \nL 351.800498 101.120243 \nL 352.100364 100.226638 \nL 352.40023 100.324779 \nL 352.999961 100.082008 \nL 353.299826 100.324779 \nL 353.599692 101.115078 \nL 353.899558 100.149158 \nL 354.199423 100.861976 \nL 354.499289 100.577881 \nL 354.799155 99.916717 \nL 355.09902 100.541724 \nL 355.398886 100.464244 \nL 355.698752 100.836149 \nL 355.998617 100.226638 \nL 356.598349 100.262795 \nL 356.898214 101.073755 \nL 357.19808 100.113 \nL 357.497946 100.593378 \nL 357.797811 100.825818 \nL 358.097677 100.288622 \nL 358.397543 100.929125 \nL 358.697408 100.815487 \nL 358.997274 100.164654 \nL 359.597005 100.696684 \nL 359.896871 100.293787 \nL 360.196736 100.965283 \nL 360.496602 100.908464 \nL 360.796468 99.766922 \nL 361.096333 100.278291 \nL 361.396199 101.037597 \nL 361.696065 100.283457 \nL 361.99593 100.459078 \nL 362.595662 100.350606 \nL 362.895527 100.732842 \nL 363.195393 100.154323 \nL 363.495259 100.314449 \nL 363.795124 100.629535 \nL 364.09499 100.443582 \nL 364.394856 100.712181 \nL 364.694721 100.107835 \nL 364.994587 100.722511 \nL 365.294453 100.593378 \nL 365.594318 101.543802 \nL 365.594318 101.543802 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 46.0125 106.036364 \nL 46.0125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.8125 106.036364 \nL 380.8125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 46.0125 106.036364 \nL 380.8125 106.036364 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 46.0125 7.2 \nL 380.8125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 46.0125 224.64 \nL 380.8125 224.64 \nL 380.8125 125.803636 \nL 46.0125 125.803636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.230682\" xlink:href=\"#m41c3de3f4b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0 -->\n      <g transform=\"translate(58.049432 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.203812\" xlink:href=\"#m41c3de3f4b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 200 -->\n      <g transform=\"translate(111.660062 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.176942\" xlink:href=\"#m41c3de3f4b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 400 -->\n      <g transform=\"translate(171.633192 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.150073\" xlink:href=\"#m41c3de3f4b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 600 -->\n      <g transform=\"translate(231.606323 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.123203\" xlink:href=\"#m41c3de3f4b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 800 -->\n      <g transform=\"translate(291.579453 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.096333\" xlink:href=\"#m41c3de3f4b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 1000 -->\n      <g transform=\"translate(348.371333 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#mac29422a27\" y=\"220.147438\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0 -->\n      <g transform=\"translate(32.65 223.946657)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#mac29422a27\" y=\"182.091004\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 2000 -->\n      <g transform=\"translate(13.5625 185.890223)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.0125\" xlink:href=\"#mac29422a27\" y=\"144.034571\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 4000 -->\n      <g transform=\"translate(13.5625 147.83379)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#p82febded82)\" d=\"M 61.230682 215.656779 \nL 61.530547 215.314271 \nL 61.830413 216.094428 \nL 62.130279 215.54261 \nL 62.73001 216.893613 \nL 63.029876 217.825996 \nL 63.329741 211.032922 \nL 63.629607 218.377814 \nL 63.929473 215.066904 \nL 64.229338 209.739003 \nL 64.529204 210.500132 \nL 64.82907 216.722359 \nL 65.128935 216.265682 \nL 65.428801 216.532077 \nL 65.728667 217.007782 \nL 66.028532 209.605806 \nL 66.328398 219.119914 \nL 66.628264 212.174615 \nL 66.928129 217.559601 \nL 67.227995 215.181073 \nL 67.527861 214.724396 \nL 67.827726 216.950698 \nL 68.127592 216.246654 \nL 68.427457 214.648283 \nL 68.727323 216.874585 \nL 69.327054 212.174615 \nL 69.926786 218.092391 \nL 70.226651 215.942202 \nL 70.526517 215.942202 \nL 70.826383 217.236121 \nL 71.126248 216.646246 \nL 71.426114 210.30985 \nL 71.72598 216.208597 \nL 72.025845 214.724396 \nL 72.325711 211.565712 \nL 72.625577 213.08797 \nL 72.925442 212.479067 \nL 73.225308 215.066904 \nL 73.525174 213.639788 \nL 73.825039 215.409412 \nL 74.124905 215.390384 \nL 74.42477 216.570133 \nL 74.724636 215.866089 \nL 75.024502 214.705368 \nL 75.324367 218.111419 \nL 75.624233 217.67377 \nL 75.924099 214.933707 \nL 76.223964 210.918753 \nL 76.52383 215.143017 \nL 76.823696 216.8175 \nL 77.123561 211.413487 \nL 77.423427 214.971763 \nL 77.723293 212.726433 \nL 78.023158 216.741387 \nL 78.323024 216.322766 \nL 78.62289 215.409412 \nL 78.922755 215.942202 \nL 79.222621 217.67377 \nL 79.522487 217.331262 \nL 79.822352 215.923174 \nL 80.122218 215.942202 \nL 80.422084 217.045839 \nL 80.721949 210.614301 \nL 81.32168 219.576592 \nL 81.921412 211.90822 \nL 82.221277 212.288784 \nL 82.521143 217.845024 \nL 82.821009 218.587124 \nL 83.42074 210.652358 \nL 83.720606 214.705368 \nL 84.020471 216.379851 \nL 84.320337 215.942202 \nL 84.620203 216.551105 \nL 84.920068 216.018315 \nL 85.219934 217.825996 \nL 85.819665 214.591199 \nL 86.119531 215.923174 \nL 86.419397 214.705368 \nL 86.719262 216.436936 \nL 87.019128 217.160008 \nL 87.318994 210.766527 \nL 87.618859 215.104961 \nL 88.21859 215.999287 \nL 88.518456 213.259224 \nL 88.818322 215.295243 \nL 89.118187 215.276215 \nL 89.418053 214.990791 \nL 89.717919 218.187532 \nL 90.017784 218.244616 \nL 90.31765 214.819537 \nL 90.617516 215.409412 \nL 90.917381 216.893613 \nL 91.217247 217.616685 \nL 91.517113 215.523581 \nL 91.816978 215.409412 \nL 92.116844 215.828033 \nL 92.716575 214.515086 \nL 93.016441 216.303738 \nL 93.316307 219.842987 \nL 93.616172 219.386309 \nL 93.916038 215.047876 \nL 94.215903 218.967689 \nL 94.515769 215.713864 \nL 94.815635 215.21913 \nL 95.1155 215.181073 \nL 95.415366 217.845024 \nL 95.715232 217.559601 \nL 96.314963 218.41587 \nL 96.614829 214.838566 \nL 96.914694 219.59562 \nL 97.21456 213.639788 \nL 97.814291 217.730854 \nL 98.114157 215.085932 \nL 98.414023 214.03938 \nL 98.713888 215.371356 \nL 99.013754 219.176999 \nL 99.31362 210.880696 \nL 99.613485 213.373393 \nL 99.913351 217.711826 \nL 100.213217 215.942202 \nL 100.513082 217.179036 \nL 100.812948 217.179036 \nL 101.112813 215.390384 \nL 101.412679 216.570133 \nL 101.712545 214.419945 \nL 102.01241 218.035306 \nL 102.312276 215.637751 \nL 102.612142 215.561638 \nL 102.912007 217.369318 \nL 103.211873 215.942202 \nL 103.511739 217.578629 \nL 103.811604 217.483488 \nL 104.11147 219.500479 \nL 104.411336 211.05195 \nL 104.711201 216.341795 \nL 105.011067 217.045839 \nL 105.310933 215.21913 \nL 105.610798 217.464459 \nL 105.910664 218.396842 \nL 106.510395 215.847061 \nL 106.810261 209.225242 \nL 107.110127 216.779444 \nL 107.409992 219.043801 \nL 108.009723 209.662891 \nL 108.309589 215.447468 \nL 108.609455 218.587124 \nL 108.90932 217.67377 \nL 109.209186 215.028848 \nL 109.509052 215.447468 \nL 109.808917 215.409412 \nL 110.108783 215.238158 \nL 110.408649 215.656779 \nL 110.708514 215.694835 \nL 111.00838 213.925211 \nL 111.308246 210.462076 \nL 111.608111 210.84264 \nL 111.907977 217.464459 \nL 112.207843 216.969726 \nL 112.507708 214.724396 \nL 112.807574 215.143017 \nL 113.10744 215.96123 \nL 113.407305 214.629255 \nL 114.007036 216.094428 \nL 114.306902 214.438973 \nL 114.606768 216.646246 \nL 114.906633 210.481104 \nL 115.206499 218.016278 \nL 115.506365 216.874585 \nL 115.80623 219.957156 \nL 116.106096 212.574208 \nL 116.405962 216.49402 \nL 116.705827 218.453927 \nL 117.305559 211.965305 \nL 117.605424 216.874585 \nL 117.90529 217.388347 \nL 118.205156 215.675807 \nL 118.505021 216.113456 \nL 118.804887 218.644209 \nL 119.104753 212.174615 \nL 119.404618 211.128063 \nL 119.704484 218.054334 \nL 120.00435 218.016278 \nL 120.304215 215.713864 \nL 120.604081 216.741387 \nL 120.903946 214.838566 \nL 121.203812 216.056371 \nL 121.503678 211.356402 \nL 121.803543 216.474992 \nL 122.103409 216.037343 \nL 122.403275 213.715901 \nL 122.70314 213.944239 \nL 123.003006 213.030885 \nL 123.302872 212.9738 \nL 123.602737 212.55518 \nL 124.202469 218.73935 \nL 124.502334 217.578629 \nL 124.8022 215.828033 \nL 125.102066 216.988754 \nL 125.401931 212.954772 \nL 126.001663 219.976184 \nL 126.301528 216.417908 \nL 126.601394 218.511011 \nL 126.90126 216.208597 \nL 127.201125 210.899725 \nL 127.500991 219.157971 \nL 127.800856 218.244616 \nL 128.100722 216.189569 \nL 128.400588 215.352327 \nL 128.700453 218.625181 \nL 129.000319 219.386309 \nL 129.300185 212.688377 \nL 129.60005 216.589161 \nL 129.899916 216.303738 \nL 130.199782 213.925211 \nL 130.499647 214.077437 \nL 130.799513 217.978221 \nL 131.099379 218.53004 \nL 131.399244 217.007782 \nL 131.69911 218.130447 \nL 131.998976 216.417908 \nL 132.598707 215.485525 \nL 133.198438 217.845024 \nL 133.79817 211.660853 \nL 134.098035 217.502516 \nL 134.397901 215.181073 \nL 134.697766 214.134522 \nL 134.997632 216.170541 \nL 135.297498 215.21913 \nL 135.597363 218.986717 \nL 135.897229 210.233737 \nL 136.49696 217.236121 \nL 136.796826 217.711826 \nL 137.096692 216.912641 \nL 137.396557 216.703331 \nL 137.696423 215.770948 \nL 137.996289 216.360823 \nL 138.296154 214.115493 \nL 138.59602 217.787939 \nL 138.895886 213.772985 \nL 139.195751 213.106998 \nL 139.495617 216.49402 \nL 139.795483 215.162045 \nL 140.095348 218.073362 \nL 140.395214 212.44101 \nL 140.695079 214.096465 \nL 140.994945 217.921137 \nL 141.594676 210.63333 \nL 142.194408 216.056371 \nL 142.494273 215.637751 \nL 142.794139 216.436936 \nL 143.094005 215.523581 \nL 143.39387 215.276215 \nL 143.693736 215.409412 \nL 143.993602 214.591199 \nL 144.293467 211.851135 \nL 144.593333 214.03938 \nL 144.893199 217.67377 \nL 145.193064 211.16612 \nL 145.792796 215.828033 \nL 146.092661 215.390384 \nL 146.392527 216.893613 \nL 146.692393 216.341795 \nL 146.992258 215.504553 \nL 147.292124 218.796435 \nL 147.591989 216.227625 \nL 147.891855 215.847061 \nL 148.191721 217.806967 \nL 148.791452 213.259224 \nL 149.091318 211.16612 \nL 149.391183 216.113456 \nL 149.990915 214.648283 \nL 150.590646 210.918753 \nL 150.890512 217.787939 \nL 151.190377 219.157971 \nL 151.490243 215.181073 \nL 151.790109 217.616685 \nL 152.089974 217.825996 \nL 152.689706 214.990791 \nL 152.989571 211.413487 \nL 153.289437 217.540572 \nL 153.589303 211.489599 \nL 153.889168 212.003361 \nL 154.189034 219.043801 \nL 154.488899 216.532077 \nL 154.788765 216.779444 \nL 155.088631 214.914678 \nL 155.388496 215.980259 \nL 155.688362 216.265682 \nL 155.988228 216.779444 \nL 156.288093 215.770948 \nL 156.587959 215.675807 \nL 157.18769 216.760415 \nL 157.787422 215.21913 \nL 158.087287 217.483488 \nL 158.387153 218.339757 \nL 158.687019 212.536151 \nL 158.986884 218.149475 \nL 159.586616 212.55518 \nL 159.886481 217.902108 \nL 160.186347 214.990791 \nL 160.486212 216.0754 \nL 160.786078 214.210634 \nL 161.085944 214.381888 \nL 161.385809 216.8175 \nL 161.685675 215.637751 \nL 161.985541 217.293205 \nL 162.285406 214.971763 \nL 162.585272 218.549068 \nL 162.885138 213.468534 \nL 163.185003 217.959193 \nL 163.484869 218.111419 \nL 163.784735 215.637751 \nL 164.0846 211.261261 \nL 164.384466 214.971763 \nL 164.684332 216.094428 \nL 164.984197 217.921137 \nL 165.284063 216.760415 \nL 165.583929 216.322766 \nL 165.883794 214.781481 \nL 166.18366 214.648283 \nL 166.483526 216.360823 \nL 166.783391 214.89565 \nL 167.083257 215.295243 \nL 167.383122 218.41587 \nL 167.982854 216.893613 \nL 168.282719 217.35029 \nL 168.582585 214.553142 \nL 168.882451 214.343832 \nL 169.182316 213.487562 \nL 169.482182 213.982296 \nL 169.782048 218.282673 \nL 170.081913 215.54261 \nL 170.381779 218.377814 \nL 170.681645 213.887155 \nL 170.98151 211.813079 \nL 171.281376 213.202139 \nL 171.581242 217.007782 \nL 171.881107 215.656779 \nL 172.180973 219.157971 \nL 172.480839 216.893613 \nL 172.780704 212.022389 \nL 173.08057 211.470571 \nL 173.380436 215.904146 \nL 173.980167 218.549068 \nL 174.280032 215.980259 \nL 174.579898 215.770948 \nL 174.879764 214.800509 \nL 175.179629 216.836528 \nL 175.479495 214.800509 \nL 175.779361 215.656779 \nL 176.079226 217.274177 \nL 176.379092 215.732892 \nL 176.678958 218.377814 \nL 176.978823 214.990791 \nL 177.578555 215.599694 \nL 177.87842 215.009819 \nL 178.178286 215.257186 \nL 178.478152 211.565712 \nL 178.778017 214.57217 \nL 179.077883 214.800509 \nL 179.377749 214.381888 \nL 179.677614 211.185148 \nL 179.97748 218.491983 \nL 180.277345 219.976184 \nL 180.577211 219.27214 \nL 180.877077 214.496058 \nL 181.176942 214.648283 \nL 181.476808 209.739003 \nL 181.776674 215.561638 \nL 182.376405 212.954772 \nL 182.676271 211.090007 \nL 182.976136 213.639788 \nL 183.276002 217.331262 \nL 183.575868 216.950698 \nL 183.875733 212.479067 \nL 184.475465 218.244616 \nL 184.77533 212.878659 \nL 185.075196 209.529693 \nL 185.375062 216.037343 \nL 185.674927 214.724396 \nL 185.974793 218.891576 \nL 186.274659 213.221167 \nL 186.574524 216.246654 \nL 186.87439 211.717938 \nL 187.174255 215.447468 \nL 187.474121 215.580666 \nL 187.773987 214.343832 \nL 188.073852 214.191606 \nL 188.373718 216.798472 \nL 188.673584 214.305775 \nL 188.973449 219.519507 \nL 189.273315 212.022389 \nL 189.573181 215.200102 \nL 189.873046 214.629255 \nL 190.172912 210.290822 \nL 190.472778 217.02681 \nL 190.772643 214.248691 \nL 191.072509 219.253112 \nL 191.372375 215.047876 \nL 191.67224 215.066904 \nL 191.972106 215.371356 \nL 192.271972 216.379851 \nL 192.571837 210.214709 \nL 192.871703 218.320729 \nL 193.171569 215.257186 \nL 193.471434 215.352327 \nL 193.7713 215.276215 \nL 194.071165 217.426403 \nL 194.371031 218.453927 \nL 194.670897 215.54261 \nL 194.970762 215.371356 \nL 195.270628 217.14098 \nL 195.570494 217.045839 \nL 196.470091 215.295243 \nL 196.769956 216.931669 \nL 197.069822 217.67377 \nL 197.669553 218.301701 \nL 197.969419 215.580666 \nL 198.269285 211.280289 \nL 198.56915 217.083895 \nL 198.869016 215.656779 \nL 199.168882 215.847061 \nL 199.468747 212.650321 \nL 199.768613 213.963268 \nL 200.068479 211.641825 \nL 200.368344 215.980259 \nL 200.66821 217.521544 \nL 200.968075 211.128063 \nL 201.267941 216.741387 \nL 201.567807 216.912641 \nL 201.867672 216.246654 \nL 202.167538 211.489599 \nL 202.467404 210.728471 \nL 203.067135 218.092391 \nL 203.666866 211.05195 \nL 203.966732 212.003361 \nL 204.566463 217.959193 \nL 204.866329 215.162045 \nL 205.166195 216.094428 \nL 205.46606 215.637751 \nL 205.765926 216.684303 \nL 206.065792 214.667312 \nL 206.365657 215.789976 \nL 206.665523 215.694835 \nL 206.965388 210.804584 \nL 207.265254 211.660853 \nL 207.56512 214.838566 \nL 207.864985 214.324804 \nL 208.164851 215.143017 \nL 208.464717 213.62076 \nL 208.764582 211.05195 \nL 209.364314 217.045839 \nL 209.664179 216.589161 \nL 209.964045 216.779444 \nL 210.263911 216.0754 \nL 210.563776 217.198064 \nL 210.863642 213.944239 \nL 211.163508 216.760415 \nL 211.463373 215.066904 \nL 211.763239 217.255149 \nL 212.063105 215.732892 \nL 212.36297 218.054334 \nL 212.662836 214.172578 \nL 212.962702 216.341795 \nL 213.262567 216.950698 \nL 213.862298 215.162045 \nL 214.162164 216.646246 \nL 214.46203 219.119914 \nL 214.761895 214.248691 \nL 215.061761 215.923174 \nL 215.361627 218.396842 \nL 215.661492 214.876622 \nL 215.961358 210.005398 \nL 216.261224 210.51916 \nL 216.561089 209.929286 \nL 216.860955 216.665274 \nL 217.160821 216.151512 \nL 217.460686 216.513049 \nL 217.760552 217.749883 \nL 218.060418 214.077437 \nL 218.360283 214.89565 \nL 218.660149 215.085932 \nL 218.960015 216.665274 \nL 219.25988 216.627218 \nL 219.859612 214.990791 \nL 220.159477 212.669349 \nL 220.459343 216.798472 \nL 220.759208 214.515086 \nL 221.059074 215.847061 \nL 221.658805 215.21913 \nL 221.958671 217.121952 \nL 222.258537 216.379851 \nL 222.858268 213.030885 \nL 223.158134 215.656779 \nL 223.457999 214.971763 \nL 223.757865 212.574208 \nL 224.057731 215.694835 \nL 224.357596 215.54261 \nL 224.657462 210.51916 \nL 224.957328 215.352327 \nL 225.257193 216.037343 \nL 225.557059 216.436936 \nL 226.15679 214.724396 \nL 226.456656 214.134522 \nL 226.756521 215.352327 \nL 227.056387 217.293205 \nL 227.356253 210.024427 \nL 227.656118 210.595273 \nL 228.25585 217.711826 \nL 228.555715 217.312234 \nL 228.855581 212.707405 \nL 229.155447 211.204176 \nL 229.455312 215.314271 \nL 229.755178 215.123989 \nL 230.055044 214.438973 \nL 230.354909 212.345869 \nL 230.654775 214.191606 \nL 230.954641 209.567749 \nL 231.254506 215.200102 \nL 231.554372 216.246654 \nL 231.854238 215.028848 \nL 232.453969 209.891229 \nL 232.753835 210.500132 \nL 233.0537 217.845024 \nL 233.353566 215.352327 \nL 233.653431 218.720322 \nL 233.953297 212.76449 \nL 234.253163 216.893613 \nL 234.553028 214.096465 \nL 234.852894 214.058409 \nL 235.15276 218.320729 \nL 235.452625 215.675807 \nL 235.752491 215.847061 \nL 236.052357 217.312234 \nL 236.352222 215.485525 \nL 236.652088 214.667312 \nL 236.951954 218.491983 \nL 237.251819 211.984333 \nL 237.551685 212.878659 \nL 237.851551 218.396842 \nL 238.151416 213.316308 \nL 238.451282 216.151512 \nL 238.751148 214.458001 \nL 239.051013 210.785555 \nL 239.350879 217.35029 \nL 239.650745 213.145054 \nL 239.95061 215.314271 \nL 240.550341 216.037343 \nL 240.850207 216.60819 \nL 241.150073 217.426403 \nL 241.449938 216.874585 \nL 241.749804 215.143017 \nL 242.04967 215.923174 \nL 242.349535 217.616685 \nL 242.649401 216.094428 \nL 242.949267 212.022389 \nL 243.249132 214.419945 \nL 243.548998 215.54261 \nL 243.848864 215.066904 \nL 244.148729 218.377814 \nL 244.448595 216.208597 \nL 244.748461 215.181073 \nL 245.048326 211.432515 \nL 245.348192 211.242233 \nL 245.947923 217.597657 \nL 246.247789 217.540572 \nL 246.547655 214.857594 \nL 246.84752 218.187532 \nL 247.147386 214.515086 \nL 247.447251 216.969726 \nL 247.747117 214.020352 \nL 248.346848 216.760415 \nL 248.646714 215.809005 \nL 248.94658 214.15355 \nL 249.246445 215.54261 \nL 249.546311 215.866089 \nL 250.146042 213.183111 \nL 250.445908 219.919099 \nL 250.745774 212.326841 \nL 251.045639 216.8175 \nL 251.345505 219.633676 \nL 251.645371 214.857594 \nL 251.945236 218.453927 \nL 252.245102 215.866089 \nL 252.844833 212.688377 \nL 253.144699 218.054334 \nL 253.444564 215.694835 \nL 253.74443 217.692798 \nL 254.044296 215.238158 \nL 254.344161 217.692798 \nL 254.644027 213.164082 \nL 254.943893 211.299317 \nL 255.243758 219.176999 \nL 255.543624 211.204176 \nL 255.84349 214.286747 \nL 256.143355 213.411449 \nL 256.443221 210.404991 \nL 256.743087 216.227625 \nL 257.042952 217.255149 \nL 257.342818 216.969726 \nL 257.642684 215.276215 \nL 257.942549 215.371356 \nL 258.242415 215.238158 \nL 258.542281 216.474992 \nL 258.842146 214.36286 \nL 259.142012 213.944239 \nL 259.441878 213.963268 \nL 259.741743 211.565712 \nL 260.041609 217.274177 \nL 260.341474 216.893613 \nL 260.64134 215.923174 \nL 260.941206 213.868126 \nL 261.241071 214.172578 \nL 261.840803 219.747845 \nL 262.140668 210.214709 \nL 262.440534 212.060446 \nL 262.7404 217.978221 \nL 263.040265 219.348253 \nL 263.340131 215.085932 \nL 263.639997 215.809005 \nL 263.939862 217.711826 \nL 264.239728 215.047876 \nL 264.539594 213.734929 \nL 264.839459 215.409412 \nL 265.139325 218.587124 \nL 265.439191 213.963268 \nL 265.739056 213.772985 \nL 266.038922 217.14098 \nL 266.338788 219.081858 \nL 266.638653 216.551105 \nL 266.938519 218.625181 \nL 267.238384 211.013894 \nL 267.53825 210.956809 \nL 267.838116 215.54261 \nL 268.137981 215.295243 \nL 268.437847 209.700947 \nL 268.737713 215.980259 \nL 269.037578 215.21913 \nL 269.337444 211.717938 \nL 269.63731 217.521544 \nL 269.937175 210.176652 \nL 270.237041 216.722359 \nL 270.536907 211.736966 \nL 270.836772 216.132484 \nL 271.136638 214.115493 \nL 271.436504 217.045839 \nL 271.736369 214.343832 \nL 272.036235 215.409412 \nL 272.336101 215.523581 \nL 272.635966 211.641825 \nL 272.935832 210.30985 \nL 273.235697 214.667312 \nL 273.535563 215.942202 \nL 274.135294 214.134522 \nL 274.43516 215.599694 \nL 274.735026 214.743424 \nL 275.034891 217.825996 \nL 275.334757 214.800509 \nL 275.934488 215.96123 \nL 276.234354 215.96123 \nL 276.53422 216.303738 \nL 276.834085 214.743424 \nL 277.133951 217.14098 \nL 277.433817 215.694835 \nL 277.733682 216.094428 \nL 278.033548 211.37543 \nL 278.333414 216.855557 \nL 278.633279 215.009819 \nL 278.933145 217.635713 \nL 279.532876 212.174615 \nL 280.132607 206.865743 \nL 280.432473 205.552796 \nL 280.732339 208.521198 \nL 281.032204 205.876275 \nL 281.631936 182.852133 \nL 281.931801 185.915676 \nL 282.231667 215.923174 \nL 282.531533 195.601038 \nL 282.831398 198.455271 \nL 283.131264 188.598655 \nL 283.43113 188.084893 \nL 283.730995 171.378118 \nL 284.030861 165.155892 \nL 284.330727 193.584047 \nL 284.630592 194.535458 \nL 284.930458 189.321727 \nL 285.230324 147.840214 \nL 285.530189 130.296198 \nL 285.830055 140.019617 \nL 286.129921 158.210592 \nL 286.429786 167.30608 \nL 287.029517 200.719629 \nL 287.329383 173.299968 \nL 287.629249 205.895304 \nL 287.929114 143.235386 \nL 288.22898 164.356706 \nL 288.528846 205.762106 \nL 288.828711 203.973454 \nL 289.128577 162.796393 \nL 289.428443 168.295547 \nL 289.728308 195.848405 \nL 290.028174 173.433166 \nL 290.32804 203.649974 \nL 290.627905 177.181725 \nL 290.927771 182.471569 \nL 291.227637 206.504207 \nL 292.127234 178.247305 \nL 292.726965 208.407028 \nL 293.02683 214.591199 \nL 293.326696 202.641479 \nL 293.626562 201.309503 \nL 293.926427 204.201792 \nL 294.226293 192.099846 \nL 294.526159 190.025771 \nL 294.826024 204.86778 \nL 295.12589 170.103228 \nL 295.425756 183.137556 \nL 295.725621 209.662891 \nL 296.025487 202.051604 \nL 296.325353 202.546337 \nL 296.925084 216.988754 \nL 297.22495 209.815116 \nL 297.524815 196.152857 \nL 297.824681 210.081511 \nL 299.024144 185.953732 \nL 299.324009 202.698563 \nL 299.623875 207.170194 \nL 299.92374 203.022043 \nL 300.223606 193.679188 \nL 300.523472 199.501823 \nL 300.823337 210.385963 \nL 301.123203 212.2317 \nL 301.423069 211.90822 \nL 301.722934 206.066558 \nL 302.0228 209.149129 \nL 302.322666 214.020352 \nL 302.922397 204.753611 \nL 303.222263 204.620413 \nL 303.522128 210.119568 \nL 303.821994 207.075053 \nL 304.12186 202.508281 \nL 304.721591 216.379851 \nL 305.021457 206.294896 \nL 305.321322 206.351981 \nL 305.621188 215.047876 \nL 305.921054 200.890883 \nL 306.220919 207.702984 \nL 306.520785 207.132138 \nL 306.82065 215.371356 \nL 307.120516 215.485525 \nL 307.420382 217.14098 \nL 307.720247 205.07709 \nL 308.319979 211.394458 \nL 308.619844 202.641479 \nL 308.91971 203.973454 \nL 309.519441 214.477029 \nL 309.819307 218.111419 \nL 310.419038 205.362514 \nL 310.718904 215.104961 \nL 311.01877 215.599694 \nL 311.318635 217.711826 \nL 311.918367 206.028501 \nL 312.218232 210.385963 \nL 312.518098 210.138596 \nL 312.817964 216.018315 \nL 313.117829 216.303738 \nL 313.417695 214.496058 \nL 313.71756 219.100886 \nL 314.317292 209.643862 \nL 314.617157 216.360823 \nL 314.917023 211.489599 \nL 315.216889 210.176652 \nL 315.516754 211.05195 \nL 315.81662 219.234084 \nL 316.116486 218.111419 \nL 316.416351 214.248691 \nL 316.716217 215.21913 \nL 317.016083 215.047876 \nL 317.315948 217.198064 \nL 317.615814 214.990791 \nL 317.91568 211.736966 \nL 318.215545 219.138943 \nL 318.515411 217.426403 \nL 318.815277 217.217093 \nL 319.115142 218.853519 \nL 319.415008 213.525619 \nL 319.714873 215.047876 \nL 320.014739 218.149475 \nL 320.314605 215.54261 \nL 320.61447 210.119568 \nL 320.914336 210.271794 \nL 321.214202 215.047876 \nL 321.514067 213.29728 \nL 321.813933 215.580666 \nL 322.113799 212.288784 \nL 322.413664 216.60819 \nL 322.71353 215.675807 \nL 323.013396 212.155587 \nL 323.613127 218.568096 \nL 323.912993 213.316308 \nL 324.212858 217.711826 \nL 324.512724 210.005398 \nL 324.81259 215.923174 \nL 325.112455 216.28471 \nL 325.412321 210.63333 \nL 325.712187 214.400917 \nL 326.311918 216.855557 \nL 326.611783 215.904146 \nL 326.911649 216.379851 \nL 327.211515 215.504553 \nL 327.51138 215.599694 \nL 327.811246 216.893613 \nL 328.410977 213.563675 \nL 328.710843 215.181073 \nL 329.010709 215.276215 \nL 329.310574 216.28471 \nL 329.61044 215.75192 \nL 329.910306 214.914678 \nL 330.210171 215.942202 \nL 330.510037 215.104961 \nL 330.809903 215.238158 \nL 331.109768 216.246654 \nL 331.409634 215.580666 \nL 331.7095 214.001324 \nL 332.009365 214.876622 \nL 332.309231 214.667312 \nL 332.609097 212.460038 \nL 332.908962 211.37543 \nL 333.508693 218.377814 \nL 333.808559 212.383926 \nL 334.108425 216.532077 \nL 334.40829 218.625181 \nL 334.708156 214.610227 \nL 335.008022 212.269756 \nL 335.307887 212.859631 \nL 335.607753 215.789976 \nL 335.907619 216.60819 \nL 336.207484 215.96123 \nL 336.50735 215.980259 \nL 336.807216 219.119914 \nL 337.107081 218.568096 \nL 337.406947 210.119568 \nL 337.706813 210.347906 \nL 338.306544 219.823958 \nL 338.60641 218.967689 \nL 339.206141 212.2317 \nL 339.506006 215.770948 \nL 339.805872 216.665274 \nL 340.405603 210.899725 \nL 340.705469 219.100886 \nL 341.005335 214.952735 \nL 341.3052 217.502516 \nL 341.904932 216.341795 \nL 342.204797 217.940165 \nL 342.504663 214.058409 \nL 342.804529 213.335336 \nL 343.104394 215.21913 \nL 343.40426 219.367281 \nL 343.704126 212.345869 \nL 344.003991 212.117531 \nL 344.303857 218.149475 \nL 344.603723 215.162045 \nL 344.903588 213.963268 \nL 345.203454 217.502516 \nL 345.50332 217.045839 \nL 345.803185 215.257186 \nL 346.103051 210.671386 \nL 346.402916 211.185148 \nL 346.702782 213.811042 \nL 347.002648 211.546684 \nL 347.302513 217.121952 \nL 347.602379 213.963268 \nL 347.902245 219.995212 \nL 348.501976 211.679882 \nL 348.801842 211.070979 \nL 349.101707 218.796435 \nL 349.401573 214.933707 \nL 349.701439 215.485525 \nL 350.001304 214.096465 \nL 350.30117 213.544647 \nL 350.601036 215.942202 \nL 350.900901 216.513049 \nL 351.200767 217.521544 \nL 351.500633 210.195681 \nL 351.800498 215.352327 \nL 352.100364 218.130447 \nL 352.40023 216.684303 \nL 352.700095 217.236121 \nL 352.999961 213.811042 \nL 353.299826 213.50659 \nL 353.599692 216.189569 \nL 354.199423 211.185148 \nL 354.499289 218.054334 \nL 354.799155 215.54261 \nL 355.09902 219.310196 \nL 355.398886 214.857594 \nL 355.698752 213.677844 \nL 355.998617 219.900071 \nL 356.298483 216.703331 \nL 356.598349 218.016278 \nL 356.898214 213.487562 \nL 357.19808 217.217093 \nL 357.497946 218.320729 \nL 357.797811 216.151512 \nL 358.097677 219.100886 \nL 358.397543 216.627218 \nL 358.697408 217.407375 \nL 358.997274 217.521544 \nL 359.297139 216.436936 \nL 359.597005 216.49402 \nL 359.896871 215.162045 \nL 360.196736 217.825996 \nL 360.496602 213.582703 \nL 360.796468 216.589161 \nL 361.096333 217.121952 \nL 361.396199 214.762453 \nL 361.696065 214.952735 \nL 361.99593 214.477029 \nL 362.295796 215.21913 \nL 362.595662 216.874585 \nL 362.895527 213.316308 \nL 363.495259 217.121952 \nL 363.795124 217.597657 \nL 364.09499 210.424019 \nL 364.394856 217.274177 \nL 364.694721 218.511011 \nL 365.294453 212.669349 \nL 365.594318 220.147438 \nL 365.594318 220.147438 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 46.0125 224.64 \nL 46.0125 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 380.8125 224.64 \nL 380.8125 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 46.0125 224.64 \nL 380.8125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 46.0125 125.803636 \nL 380.8125 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc5a5ff81cf\">\n   <rect height=\"98.836364\" width=\"334.8\" x=\"46.0125\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p82febded82\">\n   <rect height=\"98.836364\" width=\"334.8\" x=\"46.0125\" y=\"125.803636\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Conv2d(input_channels, output_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv2d(1, 64, 1) \n",
    "        self.conv2 = nn.Conv2d(64, 128, 7)  \n",
    "        self.conv3 = nn.Conv2d(128, 128, 4)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fcOut = nn.Linear(4096, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def convs(self, x):\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        #1, 105, 105\n",
    "        print(x.dtype)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 64, 96, 96\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # 64, 48, 48\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        # 128, 42, 42\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # 128, 21, 21\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # 128, 18, 18\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        # 128, 9, 9\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        # 256, 6, 6\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.type(torch.DoubleTensor)\n",
    "        x1 = self.convs(x1)\n",
    "        x1 = x1.view(-1, 256 * 6 * 6)\n",
    "        x1 = self.sigmoid(self.fc1(x1))\n",
    "        x2 = self.convs(x2)\n",
    "        x2 = x2.view(-1, 256 * 6 * 6)\n",
    "        x2 = self.sigmoid(self.fc1(x2))\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fcOut(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "\n",
    "#creating the original network and couting the paramenters of different networks\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "siameseBaseLine = Net()\n",
    "siameseBaseLine = siameseBaseLine.to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model architecture:\\n\\n', model)\n",
    "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
    "    \n",
    "count_parameters(siameseBaseLine)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model architecture:\n",
      "\n",
      " Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (fcOut): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "The model has 38,946,561 trainable parameters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# training and validation after every epoch\n",
    "def train(model, train_loader, val_loader, num_epochs, criterion, save_name):\n",
    "    best_val_loss = float(\"Inf\") \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    cur_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        print(\"Starting epoch \" + str(epoch+1))\n",
    "        for img1, img2, labels in train_loader:\n",
    "            \n",
    "            # Forward\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(img1, img2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for img1, img2, labels in val_loader:\n",
    "                img1 = img1.to(device)\n",
    "                img2 = img2.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(img1, img2)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
    "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
    "    \n",
    "    print(\"Finished Training\")  \n",
    "    return train_losses, val_losses  \n",
    "\n",
    "# evaluation metrics\n",
    "def eval(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        print('Starting Iteration')\n",
    "        count = 0\n",
    "        for mainImg, imgSets, label in test_loader:\n",
    "            mainImg = mainImg.to(device)\n",
    "            predVal = 0\n",
    "            pred = -1\n",
    "            for i, testImg in enumerate(imgSets):\n",
    "                testImg = testImg.to(device)\n",
    "                output = model(mainImg, testImg)\n",
    "                if output > predVal:\n",
    "                    pred = i\n",
    "                    predVal = output\n",
    "            label = label.to(device)\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "            if count % 20 == 0:\n",
    "                print(\"Current Count is: {}\".format(count))\n",
    "                print('Accuracy on n way: {}'.format(correct/count))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# actual training\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(siameseBaseLine.parameters(), lr = 0.0006)\n",
    "num_epochs = 50\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "save_path = 'siameseNet-batchnorm50.pt'\n",
    "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting epoch 1\n",
      "torch.float64\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6517/1049396224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'siameseNet-batchnorm50.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiameseBaseLine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6517/1543294907.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, criterion, save_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6517/1675490572.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6517/1675490572.py\u001b[0m in \u001b[0;36mconvs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 64, 96, 96\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p1_ml/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('p1_ml': conda)"
  },
  "interpreter": {
   "hash": "45ad390018fe16e49d36e7f3f69bb2868b1f449ecdbcbe075a23198a68f4bc93"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}